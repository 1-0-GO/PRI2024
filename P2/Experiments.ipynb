{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mG11_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mG11_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mG11_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mG11_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marti\\OneDrive\\Documentos\\Universidade\\2023%2024\\Semestre2\\Information Retrieval\\Projeto\\P2\\G11_code\\helper_functions.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from G11_code.data_collection import *\n",
    "from G11_code.helper_functions import *\n",
    "from G11_code.indexing import *\n",
    "from G11_code.clustering import *\n",
    "from G11_code.evaluation import evaluation\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "_article_file_paths_by_cat, _articles_by_cat, _summary_file_paths_by_cat, _summaries_by_cat, category_names = read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_summary_sentence_indices_by_cat, faulty_summary_ids = get_summary_sentence_indices(_articles_by_cat, _summaries_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_cat = remove_entries(_articles_by_cat, faulty_summary_ids)\n",
    "articles = flatten(articles_by_cat)\n",
    "article_file_paths_by_cat = remove_entries(_article_file_paths_by_cat, faulty_summary_ids)\n",
    "article_file_paths = flatten(article_file_paths_by_cat)\n",
    "summaries_by_cat = remove_entries(_summaries_by_cat, faulty_summary_ids)\n",
    "summaries = flatten(summaries_by_cat)\n",
    "summary_file_paths_by_cat = remove_entries(_summary_file_paths_by_cat, faulty_summary_ids)\n",
    "summary_file_paths = flatten(summary_file_paths_by_cat)\n",
    "summary_sentence_indices_by_cat = remove_entries(_summary_sentence_indices_by_cat, faulty_summary_ids)\n",
    "summary_sentence_indices = flatten(summary_sentence_indices_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path_to_articleID = {path:i for i, path in enumerate(article_file_paths)}\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(article_path, 'tech', '199.txt')\n",
    "d = map_path_to_articleID(path)\n",
    "compute_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match compute_index:\n",
    "    case 0 :\n",
    "        I = InvertedIndex(0,0)\n",
    "    case 1:\n",
    "        index_path = './index/Index.pkl'\n",
    "        I = indexing(None, index_path = index_path)\n",
    "    case 2:\n",
    "        I = indexing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I.doc_to_string(600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "d=0\n",
    "b = 0.5\n",
    "k = 1\n",
    "dM = bert_compute_dissimilarity_matrix(d, file_path=sentence_embeddings_path)\n",
    "sim2diss1 = lambda S: np.exp(-k(S+b))\n",
    "sim2diss2 = lambda S: (2/np.pi) * np.arccos((1-b)*S+b)\n",
    "sim2diss3 = lambda S: b*(1-np.log(1+k*S)/np.log(1+k))\n",
    "dM2 = tf_idf_compute_dissimilarity_matrix(d, I, conversion_function=sim2diss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, Q2, Q3 = np.percentile(dM2.compressed(), [25, 50, 75])\n",
    "IQR = Q3 - Q1\n",
    "ul = np.mean(dM2) + 1.5 * IQR\n",
    "np.where(dM2 >= Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(dM.compressed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(dM2.compressed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust, (labels,_) = sentence_clustering(dM2, algorithm='agglomerative', linkage='complete', kmax=len(dM2)//2)\n",
    "n_clust, len(labels), labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust, (labels, indices) = sentence_clustering(dM2, kmax=len(dM)//2)\n",
    "n_clust, len(labels), labels, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lambda y: lambda x: x+y\n",
    "b = a(1)\n",
    "b(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TextBlob('Hello, I\\'m Inigo Montoya. You killed my wife. Prepare to die. You are dead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tokenizer = RegexpTokenizer(r'[\\w|-]+')\n",
    "sent = \"I'm appaled that they would refuse to permit us to obtain the state-of-the-art refuse permit\"\n",
    "a = nltk.word_tokenize(sent)\n",
    "a = tokenizer.tokenize(sent)\n",
    "for term in a:\n",
    "    print(term.lower())\n",
    "    break\n",
    "c = Counter(nltk.pos_tag(a))\n",
    "c_ = defaultdict(list)\n",
    "for ((term,pos), count) in c.items():\n",
    "    c_[term].append((pos, count))\n",
    "c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cl, clusters = transform_labels(labels)\n",
    "sorted(zip(clusters, indices), key = lambda tup: len(tup[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silh = silhouette_samples(dM2, labels, metric='precomputed')\n",
    "silh_cl = [(silh[clust], clust[np.argmax(silh[clust])]) for clust in clusters]\n",
    "silh_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(dM2, labels, metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization(dM2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = transform_labels(labels)[1]\n",
    "len([c for c in clusters if len(c) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_function(d, embed_method, clustering_algorithm, remove_outliers, find_subtopics, evaluate=None, linkage=None, ignore_medoid_centers=False):\n",
    "    match embed_method:\n",
    "        case 'tfidf':\n",
    "            dM = tf_idf_compute_dissimilarity_matrix(d, I)\n",
    "        case 'bert':\n",
    "            dM = bert_compute_dissimilarity_matrix(d, file_path=sentence_embeddings_path)\n",
    "    kmax=len(dM)//2\n",
    "\n",
    "    n_clust, (labels, cluster_centers) = sentence_clustering(dM, algorithm=clustering_algorithm, kmax=kmax, linkage=linkage, evaluate=evaluate)\n",
    "\n",
    "    cluster_centers = (not ignore_medoid_centers and cluster_centers) or None    \n",
    "    return summarization(dM, labels, remove_outliers=remove_outliers, find_subtopics=find_subtopics, cluster_centers=cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_method = 'tfidf'\n",
    "clustering_algorithm = 'agglomerative'\n",
    "remove_outliers = True\n",
    "find_subtopics = False\n",
    "evaluate = lambda dm,labs: silhouette_score(dm, labs, metric='precomputed')\n",
    "linkage = 'average'\n",
    "ignore_medoid_centers = True\n",
    "args = (embed_method, clustering_algorithm, remove_outliers, find_subtopics, evaluate, linkage, ignore_medoid_centers)\n",
    "summarize = lambda d: summarization_function(d, *args)\n",
    "\n",
    "all_accs = summary_compute(article_file_paths_by_cat, summarize, map_path_to_articleID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust, (labels,_) = sentence_clustering(dM2, algorithm='k-medoids', linkage='complete', kmax=len(dM2)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = keyword_extraction(0, labels, I)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the parameters for the sim2diss func via regression (just for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = [tf_idf_compute_dissimilarity_matrix(d_i, I, conversion_function=lambda S: S).compressed() for d_i in tqdm(range(len(articles)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [np.log(bert_compute_dissimilarity_matrix(d_i, file_path=sentence_embeddings_path).compressed()) for d_i in tqdm(range(len(articles)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail = set([761, 1182, 1757])\n",
    "a1 = np.array([np.mean(sims) for i,sims in enumerate(ss) if i not in fail])\n",
    "b = np.array([np.mean(diss) for i,diss in enumerate(ds) if i not in fail])\n",
    "a2 = np.ones_like(a1)\n",
    "a = np.c_[a1, a2]\n",
    "x = np.linalg.lstsq(a,b,rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst = -x[0][0]\n",
    "snd = -x[0][1]/fst\n",
    "fst,snd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "s_train = np.array(flatten([sims for i,sims in enumerate(ss) if i not in fail]))\n",
    "d_train = np.array(flatten([diss for i,diss in enumerate(ds) if i not in fail]))\n",
    "def fun(x, s, d):\n",
    "    return np.exp(-x[0]*(s+x[1])) - d\n",
    "res = scipy.optimize.least_squares(fun, np.array([1.5,1.5]), args=(s_train,d_train))\n",
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([random() for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 1)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y\n",
    "\n",
    "# define problem properties\n",
    "n_timesteps = 10\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "for epoch in range(1000):\n",
    "\t# generate new random sequence\n",
    "\tX,y = get_sequence(n_timesteps)\n",
    "\t# fit model for one epoch on this sequence\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "\tprint('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import random\n",
    "import tqdm\n",
    "import numpy.random as rnd \n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([(random(),random()) for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x[0] < limit else 1 for x in cumsum(X,axis=0)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 2)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(None, 2)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# train LSTM\n",
    "for epoch in range(500):\n",
    "    #print(epoch, n_timesteps)\n",
    "    n_timesteps = rnd.randint(2,20)\n",
    "\t# generate new random sequence\n",
    "    X,y = get_sequence(n_timesteps)\n",
    "    print(X.shape, y.shape)\n",
    "\t# fit model for one epoch on this sequence\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    " keras.layers.SimpleRNN(input_shape= (None, 2),units=50, return_sequences= True),\n",
    " keras.layers.Dropout(0.2),\n",
    " keras.layers.SimpleRNN(units=50, return_sequences= True),\n",
    " keras.layers.Dropout(0.2),\n",
    " keras.layers.SimpleRNN(units=50, return_sequences= True),\n",
    " keras.layers.Dropout(0.2),\n",
    " keras.layers.Flatten(),\n",
    " keras.layers.Dense(10, activation='relu'),\n",
    " keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    " optimizer='adam',\n",
    " metrics=['accuracy'])\n",
    "model.build()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
