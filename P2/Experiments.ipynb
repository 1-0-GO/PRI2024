{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from G11_code.data_collection import *\n",
    "from G11_code.helper_functions import *\n",
    "from G11_code.indexing import *\n",
    "from G11_code.clustering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article path: ..\\BBC News Summary\\BBC News Summary\\News Articles\n",
      "Summary path: ..\\BBC News Summary\\BBC News Summary\\Summaries\n",
      "Number of Categories: 5\n",
      "Number of Articles in 'business' Category: 510\n",
      "Number of Articles in 'entertainment' Category: 386\n",
      "Number of Articles in 'politics' Category: 417\n",
      "Number of Articles in 'sport' Category: 511\n",
      "Number of Articles in 'tech' Category: 401\n"
     ]
    }
   ],
   "source": [
    "article_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "_article_file_paths_by_cat, _articles_by_cat, _summary_file_paths_by_cat, _summaries_by_cat, category_names = read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of found summaries: 2220\n",
      "number of summaries: 2225\n",
      "99.78%\n"
     ]
    }
   ],
   "source": [
    "_summary_sentence_indices_by_cat, faulty_summary_ids = get_summary_sentence_indices(_articles_by_cat, _summaries_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_cat = remove_entries(_articles_by_cat, faulty_summary_ids)\n",
    "articles = flatten(articles_by_cat)\n",
    "article_file_paths_by_cat = remove_entries(_article_file_paths_by_cat, faulty_summary_ids)\n",
    "article_file_paths = flatten(article_file_paths_by_cat)\n",
    "summaries_by_cat = remove_entries(_summaries_by_cat, faulty_summary_ids)\n",
    "summaries = flatten(summaries_by_cat)\n",
    "summary_file_paths_by_cat = remove_entries(_summary_file_paths_by_cat, faulty_summary_ids)\n",
    "summary_file_paths = flatten(summary_file_paths_by_cat)\n",
    "summary_sentence_indices_by_cat = remove_entries(_summary_sentence_indices_by_cat, faulty_summary_ids)\n",
    "summary_sentence_indices = flatten(summary_sentence_indices_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path_to_articleID = {path:i for i, path in enumerate(article_file_paths)}\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(article_path, 'tech', '199.txt')\n",
    "d = map_path_to_articleID(path)\n",
    "compute_index = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_index:\n",
    "    index_path = './index/Index.pkl'\n",
    "    I = indexing(None, index_path = index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dM = bert_compute_dissimilarity_matrix(d, file_path=sentence_embeddings_path)\n",
    "sentence_clustering(dM, algorithm='k-medoids')\n",
    "n_clust, labels = sentence_clustering(dM, kmax=len(dM)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11797707"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(dM, labels, metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([random() for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 1)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y\n",
    "\n",
    "# define problem properties\n",
    "n_timesteps = 10\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "for epoch in range(1000):\n",
    "\t# generate new random sequence\n",
    "\tX,y = get_sequence(n_timesteps)\n",
    "\t# fit model for one epoch on this sequence\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "\tprint('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import random\n",
    "import tqdm\n",
    "import numpy.random as rnd \n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([(random(),random()) for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x[0] < limit else 1 for x in cumsum(X,axis=0)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 2)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(None, 2)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "for epoch in range(50):\n",
    "    #print(epoch, n_timesteps)\n",
    "    n_timesteps = rnd.randint(2,20)\n",
    "\t# generate new random sequence\n",
    "    X,y = get_sequence(n_timesteps)\n",
    "\t# fit model for one epoch on this sequence\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
