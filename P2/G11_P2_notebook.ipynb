{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: second\n",
    "    project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 11**\n",
    "- Francisco Martins, 99068\n",
    "- Tunahan Güneş, 108108\n",
    "- Sebastian Weidinger, 111612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from G11_code.data_collection import *\n",
    "from G11_code.helper_functions import *\n",
    "from G11_code.indexing import *\n",
    "from G11_code.evaluation import *\n",
    "from G11_code.clustering import *\n",
    "from G11_code.supervised_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "_article_file_paths_by_cat, _articles_by_cat, _summary_file_paths_by_cat, _summaries_by_cat, category_names = read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_summary_sentence_indices_by_cat, faulty_summary_ids = get_summary_sentence_indices(_articles_by_cat, _summaries_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_cat = remove_entries(_articles_by_cat, faulty_summary_ids)\n",
    "articles = flatten(articles_by_cat)\n",
    "article_file_paths_by_cat = remove_entries(_article_file_paths_by_cat, faulty_summary_ids)\n",
    "article_file_paths = flatten(article_file_paths_by_cat)\n",
    "summaries_by_cat = remove_entries(_summaries_by_cat, faulty_summary_ids)\n",
    "summaries = flatten(summaries_by_cat)\n",
    "summary_file_paths_by_cat = remove_entries(_summary_file_paths_by_cat, faulty_summary_ids)\n",
    "summary_file_paths = flatten(summary_file_paths_by_cat)\n",
    "summary_sentence_indices_by_cat = remove_entries(_summary_sentence_indices_by_cat, faulty_summary_ids)\n",
    "summary_sentence_indices = flatten(summary_sentence_indices_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path_to_articleID = {path:i for i, path in enumerate(article_file_paths)}\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = indexing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(article_path, 'tech', '199.txt')\n",
    "d = map_path_to_articleID(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I.doc_to_string(d))\n",
    "dissimilarity_matrix_tfidf = tf_idf_compute_dissimilarity_matrix(d, I)\n",
    "plt.matshow(dissimilarity_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using another function to map the similarity matrix into a dissimilarity matrix such as d_ij = (1-s_ij^0.5)^2\n",
    "plt.matshow((1-np.power(1 - dissimilarity_matrix_tfidf, 1/2))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = np.array([len(nltk.sent_tokenize(articles[i]))/len(summary_sentence_indices[i]) for i in range(len(articles))])\n",
    "plt.boxplot(ratios)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "bert_model = DistilBertModel.from_pretrained(pretrained_weights)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_path = './embeddings/sentence_embeddings.pkl'\n",
    "bert_params = (bert_tokenizer, bert_model, device)\n",
    "dissimilarity_matrix_bert = bert_compute_dissimilarity_matrix(0, articles, bert_params, sentence_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(dissimilarity_matrix_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print two closest sentences according to bert\n",
    "for d in (0, 10, 600, 610, 900, 910):\n",
    "    bert_params = (bert_tokenizer, bert_model, device)\n",
    "    dissimilarity_matrix_bert = bert_compute_dissimilarity_matrix(d, articles, bert_params)\n",
    "    index_closest_bert = np.argmin(dissimilarity_matrix_bert)\n",
    "    tokenized_article = nltk.sent_tokenize(articles[d])\n",
    "    num_sent = len(tokenized_article)\n",
    "    sent1 = index_closest_bert//num_sent\n",
    "    sent2 = index_closest_bert%num_sent\n",
    "    print(tokenized_article[sent1])\n",
    "    print(tokenized_article[sent2])\n",
    "    print(dissimilarity_matrix_bert[sent1, sent2])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print two closest sentences according to tfidf\n",
    "for d in (0, 10, 600, 610, 900, 910):\n",
    "    dissimilarity_matrix_tfidf = tf_idf_compute_dissimilarity_matrix(d, I)\n",
    "    index_closest_tfidf = np.argmin(dissimilarity_matrix_tfidf)\n",
    "    tokenized_article = nltk.sent_tokenize(articles[d])\n",
    "    num_sent = len(tokenized_article)\n",
    "    sent1 = index_closest_tfidf//num_sent\n",
    "    sent2 = index_closest_tfidf%num_sent\n",
    "    print(tokenized_article[sent1])\n",
    "    print(tokenized_article[sent2])\n",
    "    print(dissimilarity_matrix_tfidf[sent1, sent2])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentence and document embeddings \n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "bert_model = DistilBertModel.from_pretrained(pretrained_weights)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "path = './embeddings'\n",
    "if not os.listdir(path): \n",
    "    save_embeddings(articles, bert_tokenizer, bert_model, device, path)\n",
    "else: \n",
    "    print(f'Files exist in folder {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from G11_code.training import *\n",
    "\n",
    "train_emb_by_cat, test_emb_by_cat, train_ind_by_cat, test_ind_by_cat = split_by_cat(sentence_embeddings_by_cat, summary_sentence_indices_by_cat)\n",
    "\n",
    "X_train, Y_train = get_XY(train_emb_by_cat, train_ind_by_cat)\n",
    "X_test, Y_test = get_XY(test_emb_by_cat, test_ind_by_cat)\n",
    "X_train = flatten(X_train)\n",
    "X_test = flatten(X_test)\n",
    "Y_train = flatten(Y_train)\n",
    "Y_test = flatten(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = reducer.fit(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = fit_PCA(X_train, n_components=n_components)\n",
    "X_train_trans = transform_PCA(pca, X_train)\n",
    "X_test_trans = transform_PCA(pca, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"XGBoost\", use_pca=True, n_components=n_components)\n",
    "model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"XGBoost\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.62      0.75      0.68      4970\n",
    "           1       0.52      0.37      0.43      3609\n",
    "\n",
    "    accuracy                           0.59      8579\n",
    "   macro avg       0.57      0.56      0.55      8579\n",
    "weighted avg       0.58      0.59      0.57      8579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, model, model_name=\"XGBoost\", use_pca=True, X_train=X_train, n_components=n_components)\n",
    "precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, model, model_name=\"XGBoost\")\n",
    "print(f'Classifier XGBoost - Precision: {precision} | Recall: {recall} | AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with variable output\n",
    "# how can be every sentence in input sequence be classified? \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Dense, Bidirectional\n",
    "\n",
    "LSTM_units = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=LSTM_units, \n",
    "               input_shape=(None, 768), \n",
    "               return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "#print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_XY(train_emb_by_cat, train_ind_by_cat)\n",
    "pbar = tqdm(total=len(X_train))\n",
    "for X, y in tqdm(zip(X_train, Y_train), total=len(X_train)):\n",
    "    #X = np.array(transform_PCA(pca, X))\n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    y = np.array(y)\n",
    "    y = y.reshape(1, len(y), 1)\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=0)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_XY(test_emb_by_cat, test_ind_by_cat)\n",
    "\n",
    "for X, y in zip(X_test, y_test): \n",
    "    #X = np.array(transform_PCA(pca, X))\n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    #y = np.array(y)\n",
    "    #y = y.reshape(1, len(y), 1)\n",
    "    y_hat = model.predict(X)\n",
    "    print(y_hat)\n",
    "    t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"NN\", use_pca=True, n_components=n_components)\n",
    "predictions = np.round(model.predict(np.array(X_test_trans)))\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metric = model.evaluate(np.array(X_test_trans), np.array(Y_test))\n",
    "print(loss_and_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.array(X_test_trans))\n",
    "print(np.round(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, model, model_name=\"NN\", use_pca=True, X_train=X_train, n_components=n_components)\n",
    "print(f'Classifier NN - Precision: {precision} | Recall: {recall} | AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Main facilities</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part I: clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Summarization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Keyword extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part II: classification</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A) Feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "sentence_embeddings = flatten(sentence_embeddings_by_cat)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)\n",
    "document_embeddings = flatten(document_embeddings_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = get_dataframe(docs=range(len(articles)), sent_embeddings=sentence_embeddings, doc_embeddings=document_embeddings,  I=I, article_file_paths=article_file_paths, articles=articles, k=0.2, b=0.75, p_keywords=10)\n",
    "df = pd.DataFrame(data)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Ranking extension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Question materials (optional)</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: clustering</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Do clustering-guided summarization alters the behavior and efficacy of the IR system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** How sentence representations, clustering choices, and rank criteria impact summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
