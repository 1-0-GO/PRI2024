{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: second\n",
    "    project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 11**\n",
    "- Francisco Martins, 99068\n",
    "- Tunahan Güneş, 108108\n",
    "- Sebastian Weidinger, 111612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from G11_code.data_collection import *\n",
    "from G11_code.helper_functions import *\n",
    "from G11_code.indexing import *\n",
    "from G11_code.evaluation import *\n",
    "from G11_code.clustering import *\n",
    "from G11_code.supervised_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "_article_file_paths_by_cat, _articles_by_cat, _summary_file_paths_by_cat, _summaries_by_cat, category_names = read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_summary_file_paths_by_cat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_summary_sentence_indices_by_cat, faulty_summary_ids = get_summary_sentence_indices(_articles_by_cat, _summaries_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_summary_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(1, 247), (1, 267), (1, 351), (3, 110), (3, 138)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_summary_sentence_indices_by_cat[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0, 2, 3, 6, 7, 10, 12, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_cat = remove_entries(_articles_by_cat, faulty_summary_ids)\n",
    "articles = flatten(articles_by_cat)\n",
    "article_file_paths_by_cat = remove_entries(_article_file_paths_by_cat, faulty_summary_ids)\n",
    "article_file_paths = flatten(article_file_paths_by_cat)\n",
    "summaries_by_cat = remove_entries(_summaries_by_cat, faulty_summary_ids)\n",
    "summaries = flatten(summaries_by_cat)\n",
    "summary_file_paths_by_cat = remove_entries(_summary_file_paths_by_cat, faulty_summary_ids)\n",
    "summary_file_paths = flatten(summary_file_paths_by_cat)\n",
    "summary_sentence_indices_by_cat = remove_entries(_summary_sentence_indices_by_cat, faulty_summary_ids)\n",
    "summary_sentence_indices = flatten(summary_sentence_indices_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentence_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0, 2, 3, 6, 7, 10, 12, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path_to_articleID = {path:i for i, path in enumerate(article_file_paths)}\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "bert_model = DistilBertModel.from_pretrained(pretrained_weights)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "bert_params = (bert_tokenizer, bert_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentence and document embeddings \n",
    "path = './embeddings'\n",
    "if not os.listdir(path): \n",
    "    save_embeddings(articles, bert_tokenizer, bert_model, device, path)\n",
    "else: \n",
    "    print(f'Files exist in folder {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(article_path, 'tech', '199.txt')\n",
    "d = map_path_to_articleID(path)\n",
    "compute_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match compute_index:\n",
    "    case 0 :\n",
    "        I = InvertedIndex(0,0)\n",
    "    case 1:\n",
    "        index_path = './index/Index.pkl'\n",
    "        I = indexing(None, index_path = index_path)\n",
    "    case 2:\n",
    "        I = indexing(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the ratio: size of article/size of summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = np.array([len(nltk.sent_tokenize(articles[i]))/len(summary_sentence_indices[i]) for i in range(len(articles))])\n",
    "plt.boxplot(ratios)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check different behavior of sentence similarity between BERT and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print two closest sentences according to bert\n",
    "for d in (0, 10, 600, 610, 900, 910):\n",
    "    bert_params = (bert_tokenizer, bert_model, device)\n",
    "    dissimilarity_matrix_bert = bert_compute_dissimilarity_matrix(d, articles, bert_params)\n",
    "    index_closest_bert = np.argmin(dissimilarity_matrix_bert)\n",
    "    tokenized_article = nltk.sent_tokenize(articles[d])\n",
    "    num_sent = len(tokenized_article)\n",
    "    sent1 = index_closest_bert//num_sent\n",
    "    sent2 = index_closest_bert%num_sent\n",
    "    print(tokenized_article[sent1])\n",
    "    print(tokenized_article[sent2])\n",
    "    print(dissimilarity_matrix_bert[sent1, sent2])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print two closest sentences according to tfidf\n",
    "for d in (0, 10, 600, 610, 900, 910):\n",
    "    dissimilarity_matrix_tfidf = tf_idf_compute_dissimilarity_matrix(d, I)\n",
    "    index_closest_tfidf = np.argmin(dissimilarity_matrix_tfidf)\n",
    "    tokenized_article = nltk.sent_tokenize(articles[d])\n",
    "    num_sent = len(tokenized_article)\n",
    "    sent1 = index_closest_tfidf//num_sent\n",
    "    sent2 = index_closest_tfidf%num_sent\n",
    "    print(tokenized_article[sent1])\n",
    "    print(tokenized_article[sent2])\n",
    "    print(dissimilarity_matrix_tfidf[sent1, sent2])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from G11_code.training import *\n",
    "\n",
    "train_emb_by_cat, test_emb_by_cat, train_ind_by_cat, test_ind_by_cat = split_by_cat(sentence_embeddings_by_cat, summary_sentence_indices_by_cat)\n",
    "\n",
    "X_train, Y_train = get_XY(train_emb_by_cat, train_ind_by_cat)\n",
    "X_test, Y_test = get_XY(test_emb_by_cat, test_ind_by_cat)\n",
    "X_train = flatten(X_train)\n",
    "X_test = flatten(X_test)\n",
    "Y_train = flatten(Y_train)\n",
    "Y_test = flatten(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Articles in 'tech' Category: 510\n",
    "Number of Articles in 'entertainment' Category: 386\n",
    "Number of Articles in 'sport' Category: 417\n",
    "Number of Articles in 'business' Category: 511\n",
    "Number of Articles in 'politics' Category: 401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize documents by category\n",
    "categories = ['tech', 'entertainment', 'sport', 'business', 'politics']\n",
    "all_doc_emb = flatten(flatten(document_embeddings_by_cat))\n",
    "#reducer = fit_UMAP(X_train, n_components=2)\n",
    "reducer = fit_UMAP(all_doc_emb, n_components=2)\n",
    "for i, cat in enumerate(document_embeddings_by_cat): \n",
    "    samples = np.array(flatten(cat)) \n",
    "    #idx = np.random.randint(samples.shape[0], size=100)\n",
    "    #samples = samples[idx,:]\n",
    "    X_train_trans = transform_UMAP(reducer, samples)\n",
    "    plt.scatter(X_train_trans[:,0], X_train_trans[:,1], label=categories[i])\n",
    "plt.legend(categories)\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()\n",
    "#X_test_trans = transform_UMAP(reducer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = fit_PCA(X_train, n_components=n_components)\n",
    "X_train_trans = transform_PCA(pca, X_train)\n",
    "X_test_trans = transform_PCA(pca, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "xgb_model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"XGBoost\", use_pca=True, n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.62      0.75      0.68      4970\n",
    "           1       0.52      0.37      0.43      3609\n",
    "\n",
    "    accuracy                           0.59      8579\n",
    "   macro avg       0.57      0.56      0.55      8579\n",
    "weighted avg       0.58      0.59      0.57      8579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(X_test_trans)\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, xgb_model, model_name=\"XGBoost\", use_pca=True, X_train=X_train, n_components=n_components)\n",
    "#precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, model, model_name=\"XGBoost\")\n",
    "print(f'Classifier XGBoost - Precision: {precision} | Recall: {recall} | AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.listdir('./nn_model'):\n",
    "    nn_model = keras.models.load_model('./nn_model/final_model')\n",
    "    nn_model.load_weights('./nn_model/final_model')\n",
    "else:\n",
    "    nn_model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"NN\", use_pca=True, n_components=n_components)\n",
    "predictions = np.round(nn_model.predict(X_test_trans)).astype(int)\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, nn_model, model_name=\"NN\", use_pca=True, X_train=X_train, n_components=n_components)\n",
    "print(f'Classifier NN - Precision: {precision} | Recall: {recall} | AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir('./nn_model'):\n",
    "    nn_model.save('./nn_model/final_model', save_format='tf')\n",
    "    nn_model.save_weights('./nn_model/final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.listdir('./lstm_model'):\n",
    "    lstm_model = keras.models.load_model('./lstm_model/final_model')\n",
    "    lstm_model.load_weights('./lstm_model/final_model')\n",
    "else:\n",
    "    lstm_model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"LSTM\", use_pca=True, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir('./lstm_model'):\n",
    "    lstm_model.save_weights('./lstm_model/final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, auc = supervised_evaluation(test_emb_by_cat, test_ind_by_cat, lstm_model, model_name=\"LSTM\", use_pca=True, X_train=X_train, n_components=n_components)\n",
    "print(f'Classifier LSTM - Precision: {precision} | Recall: {recall} | AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.listdir('./lstm_model'):\n",
    "    lstm_model.save('./lstm_model/final_model', save_format='tf')\n",
    "    lstm_model.save_weights('./lstm_model/final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_test, y_test = get_XY(test_emb_by_cat, test_ind_by_cat)\n",
    "X_test_trans = [transform_PCA(pca, x) for x in X_test]\n",
    "all_predictions = list()\n",
    "all_ytest = np.array(flatten(y_test))\n",
    "for X, y in zip(X_test_trans, y_test): \n",
    "    #X = np.array(transform_PCA(pca, X))\n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    y = np.array(y)\n",
    "    predictions = lstm_model.predict(X, verbose=0)\n",
    "    predictions = np.round(predictions.squeeze()).astype(int)\n",
    "    all_predictions.extend(predictions)\n",
    "precision = sklearn.metrics.precision_score(all_ytest, all_predictions)\n",
    "recall = sklearn.metrics.recall_score(all_ytest, all_predictions)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(all_ytest, all_predictions)  \n",
    "auc = sklearn.metrics.auc(fpr, tpr)\n",
    "   \n",
    "print(precision)\n",
    "print(recall)\n",
    "print(auc)\n",
    "print(classification_report(all_ytest, all_predictions))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Dense, Bidirectional, Dropout\n",
    "from keras.metrics import Recall\n",
    "\n",
    "LSTM_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=LSTM_units, \n",
    "               input_shape=(None, n_components), \n",
    "               return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(units=LSTM_units, return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(units=LSTM_units, return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"AUC\"])\n",
    "model.build(input_shape=(None, None, n_components))\n",
    "print(model.summary())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from itertools import groupby\n",
    "X_train, Y_train = get_XY(train_emb_by_cat, train_ind_by_cat)\n",
    "X_train_trans = [transform_PCA(pca, x) for x in X_train]\n",
    "Y_train = [y for _, y in sorted(zip(X_train_trans, Y_train), key=lambda x: len(x[0]))]\n",
    "X_train_trans.sort(key=len)\n",
    "X_train_groups = [list(g) for k, g in groupby(X_train_trans, key=len)]\n",
    "Y_train_groups = [list(g) for k, g in groupby(Y_train, key=len)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_test, y_test = get_XY(test_emb_by_cat, test_ind_by_cat)\n",
    "X_test_trans = [transform_PCA(pca, x) for x in X_test]\n",
    "\n",
    "y_t = list()\n",
    "all_predictions = list()\n",
    "n = 100  # for 2 random indices\n",
    "index = np.random.choice(len(X_test_trans), n, replace=False)\n",
    "for k in index: \n",
    "    y_t.extend(y_test[k])\n",
    "\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    for X, y in zip(X_train_groups, Y_train_groups):\n",
    "        X = np.array(X)\n",
    "        X = X.reshape(len(X), len(X[0]), n_components)\n",
    "        y = np.array(y)\n",
    "        y = y.reshape(len(y), len(y[0]), 1)\n",
    "        history = model.fit(X, y, epochs=1, batch_size=32, verbose=0, shuffle=True)\n",
    "    avg_loss = np.mean(history.history['loss'][-100:])\n",
    "    avg_auc = np.mean(history.history['auc'][-100:])\n",
    "\n",
    "    all_predictions = list()\n",
    "    for k in index: \n",
    "        x = X_test_trans[k]\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        predictions = model.predict(x, verbose=0)\n",
    "        predictions = np.round(predictions.squeeze()).astype(int)\n",
    "        all_predictions.extend(predictions)\n",
    "    print(classification_report(y_t, all_predictions))\n",
    "    print(f\"Loss:{avg_loss} | AUC:{avg_auc}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the different dissimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix_tfidf = tf_idf_compute_dissimilarity_matrix(d, I)\n",
    "plt.matshow(dissimilarity_matrix_tfidf)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(dissimilarity_matrix_bert)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *A) Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "b = 0.5\n",
    "k = 1\n",
    "dM = bert_compute_dissimilarity_matrix(d, file_path=sentence_embeddings_path)\n",
    "sim2diss1 = lambda S: np.exp(-k(S+b))\n",
    "sim2diss2 = lambda S: (2/np.pi) * np.arccos((1-b)*S+b)\n",
    "sim2diss3 = lambda S: b*(1-np.log(1+k*S)/np.log(1+k))\n",
    "dM2 = tf_idf_compute_dissimilarity_matrix(d, I, conversion_function=sim2diss2)\n",
    "\n",
    "n_clust, labels = sentence_clustering(dM, algorithm='agglomerative', linkage='complete', kmax=len(dM)//2)\n",
    "print(n_clust, len(labels), labels)\n",
    "n_clust, labels = sentence_clustering(dM2, algorithm='agglomerative', linkage='complete', kmax=len(dM2)//2)\n",
    "print(n_clust, len(labels), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *B) Summarization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *C) Keyword extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *A) Feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"..\", \"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "_article_file_paths_by_cat, _articles_by_cat, _summary_file_paths_by_cat, _summaries_by_cat, category_names = read_files(article_path, summary_path)\n",
    "_summary_sentence_indices_by_cat, faulty_summary_ids = get_summary_sentence_indices(_articles_by_cat, _summaries_by_cat)\n",
    "articles_by_cat = remove_entries(_articles_by_cat, faulty_summary_ids)\n",
    "articles = flatten(articles_by_cat)\n",
    "article_file_paths_by_cat = remove_entries(_article_file_paths_by_cat, faulty_summary_ids)\n",
    "article_file_paths = flatten(article_file_paths_by_cat)\n",
    "summaries_by_cat = remove_entries(_summaries_by_cat, faulty_summary_ids)\n",
    "summaries = flatten(summaries_by_cat)\n",
    "summary_file_paths_by_cat = remove_entries(_summary_file_paths_by_cat, faulty_summary_ids)\n",
    "summary_file_paths = flatten(summary_file_paths_by_cat)\n",
    "summary_sentence_indices_by_cat = remove_entries(_summary_sentence_indices_by_cat, faulty_summary_ids)\n",
    "summary_sentence_indices = flatten(summary_sentence_indices_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings \n",
    "sentence_embeddings_path = os.path.join('./embeddings', 'sentence_embeddings.pkl')\n",
    "sentence_embeddings_by_cat = pickle_load(sentence_embeddings_path)\n",
    "sentence_embeddings = flatten(sentence_embeddings_by_cat)\n",
    "document_embeddings_path = os.path.join('./embeddings', 'document_embeddings.pkl')\n",
    "document_embeddings_by_cat = pickle_load(document_embeddings_path)\n",
    "document_embeddings = flatten(document_embeddings_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_by_cat = generate_doc_ids_cat()\n",
    "X_train, y_train, X_test, y_test = construct_df_and_split(doc_ids_by_cat=doc_ids_by_cat,\n",
    "                                                          summary_sentence_indices_by_cat=summary_sentence_indices_by_cat,\n",
    "                                                          sent_embeddings=sentence_embeddings,\n",
    "                                                          doc_embeddings=document_embeddings,\n",
    "                                                          article_file_paths=article_file_paths,\n",
    "                                                          articles=articles,\n",
    "                                                          train_size=0.8,\n",
    "                                                          k=0.2,\n",
    "                                                          b=0.75,\n",
    "                                                          p_keywords=10)\n",
    "#X_train.to_csv('./dataframes/X_train.csv', index=False)\n",
    "#np.save('./dataframes/y_train.npy', y_train)\n",
    "#X_test.to_csv('./dataframes/X_test.csv', index=False)\n",
    "#np.save('./dataframes/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from G11_code.data_collection import *\n",
    "from G11_code.helper_functions import *\n",
    "from G11_code.indexing import *\n",
    "from G11_code.evaluation import *\n",
    "from G11_code.clustering import *\n",
    "from G11_code.supervised_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./dataframes/X_train.csv')\n",
    "y_train = np.load('./dataframes/y_train.npy')\n",
    "X_test = pd.read_csv('./dataframes/X_test.csv')\n",
    "y_test = np.load('./dataframes/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"XGBoost\", use_pca=True, n_components=n_components)\n",
    "model = training(X_train.iloc[:,1:], y_train, model_name=\"XGBoost\", use_extracted_features=True)\n",
    "\n",
    "predictions = model.predict(X_test.iloc[:,1:])\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.80      0.78      4958\n",
    "           1       0.71      0.66      0.68      3602\n",
    "\n",
    "    accuracy                           0.74      8560\n",
    "   macro avg       0.74      0.73      0.73      8560\n",
    "weighted avg       0.74      0.74      0.74      8560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#model = training(train_emb_by_cat, train_ind_by_cat, model_name=\"XGBoost\", use_pca=True, n_components=n_components)\n",
    "model = training(X_train.iloc[:,1:], y_train, model_name=\"NN\", use_extracted_features=True,use_val=True, X_val=X_test.iloc[:,1:], y_val=y_test)\n",
    "\n",
    "predictions = np.rint(model.predict(X_test.iloc[:,1:]))\n",
    "precitions = np.squeeze(predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = indexing(articles) #Put the existing index. Only needed for char lengths of the sentences.\n",
    "supervised_summarization(d=486, M=model, p=7, l=0, o='rel', x_test=X_test, I = I)\n",
    "\n",
    "article_id = 486\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "all_accs = summarization(d=article_id, p=7, l=1000, o=\"app\", I_or_D=I, model='TF-IDF')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in all_accs.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_summarization(d=2200, M=model, p=7, l=0, o='rel', x_test=X_test, I = I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = nltk.sent_tokenize(articles[486])\n",
    "ex[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_file_paths[486]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B) Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C) Ranking extension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D) Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question materials (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: clustering</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Does clustering-guided summarization alter the behavior and efficacy of the IR system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** How sentence representations, clustering choices, and rank criteria impact summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Are anchor sentences (capturing multiple topics) included? And less relevant outlier sentences excluded? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Given a set of documents, plot the distribution of the number of keywords per document.\n",
    "Are keywords generally dissimilar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
