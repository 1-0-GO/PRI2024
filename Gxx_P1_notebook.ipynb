{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 11**\n",
    "- Francisco Martins, 99068\n",
    "- Tunahan Güneş, 108108\n",
    "- Sebastian Weidinger, 111612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(text: str) -> str: \n",
    "    text = text.split(\"\\n\\n\")\n",
    "    # remove title\n",
    "    text = text[1:]\n",
    "    text = \" \".join(text)\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    text = text.replace(\" !\", \"!\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text: str) -> str: \n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    # space is missing after the end of the sentence\n",
    "    text = re.sub(r'([a-z]|[0-9]|[A-Z]|\\%|\\'|\\)|\\?|\\!)\\.([A-Z]|\\')', r'\\1. \\2', text)\n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    text = text.replace(\" !\", \"!\")\n",
    "    text = re.sub(r'([a-z])\\?([A-Z]|\\')', r'\\1? \\2', text)\n",
    "    text = re.sub(r'([a-z])\\!([A-Z]|\\')', r'\\1! \\2', text)\n",
    "    text = re.sub(r'([a-z])\\.([A-Z]|[0-9])', r'\\1. \\2', text)\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(article_path, summary_path):\n",
    "    articles = []\n",
    "    summaries = []\n",
    "    article_file_paths = []\n",
    "    summary_file_paths = []\n",
    "    for folder in os.listdir(article_path):\n",
    "        article_category_path = os.path.join(article_path, folder)\n",
    "        summary_category_path = os.path.join(summary_path, folder)\n",
    "        articles.append([])\n",
    "        summaries.append([])\n",
    "        for file in os.listdir(article_category_path):\n",
    "            article_file_path = os.path.join(article_category_path, file)\n",
    "            summary_file_path = os.path.join(summary_category_path, file)\n",
    "            article_file_paths.append(article_file_path)\n",
    "            summary_file_paths.append(summary_file_path)\n",
    "            # articles\n",
    "            with open(article_file_path, \"r\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                text = preprocess_article(text)\n",
    "                articles[-1].append(text)\n",
    "            #summaries \n",
    "            with open(summary_file_path, \"r\", errors=\"ignore\") as f: \n",
    "                text = f.read()\n",
    "                text = preprocess_summary(text)\n",
    "                summaries[-1].append(text)\n",
    "                \n",
    "    print(\"Number of Categories:\",len(os.listdir(article_path)))\n",
    "    for i in range(len(os.listdir(article_path))):\n",
    "        print(\"Number of Articles in\", \"'\"+os.listdir(article_path)[i]+\"'\", \"Category:\",len(articles[i]))\n",
    "    \n",
    "    return article_file_paths, articles, summary_file_paths, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "article_file_paths, categorized_articles, summary_file_paths, categorized_summaries = read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examplary text. The structure of the read file is: articles[category_no][document_no]. \n",
    "print(categorized_articles[0][0])\n",
    "print(article_file_paths[508:512])\n",
    "print(categorized_summaries[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_sentence_indices(articles: list, summaries: list) -> list:\n",
    "    total_sentences = 0\n",
    "    recognized_sentences = 0\n",
    "    categorized_article_summary = list(zip(articles, summaries))\n",
    "    categorized_summary_indices = list()\n",
    "    for category in categorized_article_summary: \n",
    "        article_summary = list(zip(category[0], category[1]))\n",
    "        categorized_summary_indices.append([])\n",
    "        for article, summary in article_summary: \n",
    "            article_sents = sent_tokenize(article)\n",
    "            summary_sents = sent_tokenize(summary)\n",
    "            sentence_indices = list()\n",
    "            for summary_sent in summary_sents:\n",
    "                #len_before = len(sentence_indices)\n",
    "                for sent_index, article_sent in enumerate(article_sents): \n",
    "                    if summary_sent.strip() == article_sent.strip(): \n",
    "                        sentence_indices.append(sent_index)\n",
    "                #len_after = len(sentence_indices)\n",
    "                #if len_before - len_after == 0: \n",
    "                #    print(\"Sentence not found\")\n",
    "            if len(sentence_indices) < len(summary_sents): \n",
    "                g = 0\n",
    "            total_sentences += len(summary_sents)\n",
    "            recognized_sentences += len(sentence_indices)\n",
    "            categorized_summary_indices[-1].append(sentence_indices)\n",
    "    print(f\"Total summary sentences: {total_sentences}\")\n",
    "    print(f\"Recognized summary sentences: {recognized_sentences}\")\n",
    "    print(f\"{recognized_sentences/total_sentences * 100:.2f}% setences recognized\")\n",
    "    return categorized_summary_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_summary_sentence_indices = get_summary_sentence_indices(categorized_articles, categorized_summaries)\n",
    "print(categorized_summary_sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lists) -> list: \n",
    "    return [element for sublist in lists for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentence_indices = flatten(categorized_summary_sentence_indices)\n",
    "with open(\"./testing/reference/summary_sentence_indices.json\", \"w\") as f: \n",
    "    json.dump(summary_sentence_indices, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from typing import Union\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sklearn\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter, defaultdict\n",
    "from tabulate import tabulate\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list to get uncategorized collection\n",
    "articles = flatten(categorized_articles)\n",
    "summaries = flatten(categorized_summaries)\n",
    "N = len(articles)\n",
    "N_summaries = len(summaries)\n",
    "dict_path_to_articleID = {path:i for i, path in enumerate(article_file_paths)}\n",
    "\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index Structure \n",
    " \n",
    "Each term points to a dictionary of document identifier and the term frequency in the document.\n",
    "\n",
    "t1 -> {doc1: TF, doc5: TF, ...}\\\n",
    "t2 -> {doc7: TF, doc8: TF, ...}\\\n",
    "...\n",
    "t2 -> [DF, {doc7: [TF_(t2, doc7), {s1: TF, s4: TF, ...}], doc8: [TF_(t2, doc8), {s2: TF, s4: TF, ...}], ...}]\\\n",
    "\n",
    "use class structure\n",
    "\n",
    "TODO: \n",
    "* Optimize structure?\n",
    "    * Is there a more efficient way? \n",
    "    * Add maybe pointers to sentences and their term frequency? -> Faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermFrequencies: \n",
    "    def __init__(self) -> None:\n",
    "        self.tf_d_t = 0\n",
    "        self.sent_tf = list()\n",
    "\n",
    "    def add_sentence(self, sent_number, term_frequency):\n",
    "        self.sent_tf.append((sent_number, term_frequency))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        padding = 5 - len(str(self.tf_d_t))\n",
    "        return f'TF_d_t: {self.tf_d_t}{\" \" * padding}TF_per_sentence: {self.sent_tf}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexEntry:\n",
    "    def __init__(self) -> None:\n",
    "        self.df_term = 0\n",
    "        self.term_dict = defaultdict(TermFrequencies)\n",
    "    \n",
    "    def get_document(self, document):\n",
    "        return self.term_dict.get(document, None)\n",
    "\n",
    "    def get_or_default_document(self, document):\n",
    "        return self.term_dict[document]\n",
    "\n",
    "    def update_document(self, document, new_value):\n",
    "        self.term_dict[document] = new_value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Document Frequency: {self.df_term}\\n {\" \" * (max_width+2)} Term frequencies:\\n'\n",
    "        for doc_number, tfs in self.term_dict.items():\n",
    "            padding = 5 - len(str(doc_number))\n",
    "            out += f'{\" \" * (max_width + 3)} Doc {doc_number}{\" \" * padding}→ {tfs}\\n'\n",
    "        return out\n",
    "    \n",
    "    def calculate_df(self):\n",
    "        self.df_term = len(self.term_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, collection_size) -> None:\n",
    "        self.inverted_index = defaultdict(InvertedIndexEntry)\n",
    "        self.sentence_term_counts = list()\n",
    "        self.sentence_num_chars = list()\n",
    "        self.indexing_time = 0\n",
    "        self.N = collection_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Time to index: {self.indexing_time}\\nInverted Index:\\n'\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            padding = max_width - len(term)\n",
    "            out += f'{term} {\" \" * padding} → {entry}\\n'\n",
    "        return out\n",
    "\n",
    "    def get_or_default(self, term, document):\n",
    "        return self.inverted_index[term].get_or_default_document(document)\n",
    "    \n",
    "    def update(self, term, document, new_value):\n",
    "        self.inverted_index[term].update_document(document, new_value)\n",
    "    \n",
    "    def set_indexing_time(self, indexing_time):\n",
    "        self.indexing_time = indexing_time\n",
    "    \n",
    "    def calculate_dfs(self):\n",
    "        for entry in self.inverted_index.values():\n",
    "            entry.calculate_df()  \n",
    "    \n",
    "    def get_sentence_lengths(self, document):\n",
    "        return self.sentence_term_counts[document]\n",
    "\n",
    "    def get_document_info(self, document):          \n",
    "        info = {'Vocabulary': [], 'DF_t': [], 'TF_d_t': [], 'TF/sentence': []}\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            doc_tfs = entry.get_document(document)\n",
    "            if doc_tfs == None:\n",
    "                continue\n",
    "            info['Vocabulary'].append(term)\n",
    "            info['DF_t'].append(entry.df_term)\n",
    "            info['TF_d_t'].append(doc_tfs.tf_d_t)\n",
    "            info['TF/sentence'].append(doc_tfs.sent_tf)\n",
    "        return info\n",
    "    \n",
    "    def doc_to_string(self, document: int):\n",
    "        out = f'Document id={document} → vocabulary and term frequencies:\\n'\n",
    "        info = self.get_document_info(document)\n",
    "        table = zip(*info.values())\n",
    "        headers = info.keys()\n",
    "        return out + tabulate(table, headers, tablefmt=\"pretty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(sentence):\n",
    "    sents = list()\n",
    "    for paragraph in sentence.split('\\n '):\n",
    "        # split into sentences  \n",
    "        sents_p = nltk.sent_tokenize(paragraph)\n",
    "        for sent in sents_p:\n",
    "            sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@ input a sentence to process, a tokenizer to split it into terms, a lemmatizer to normalize the terms,\n",
    "a set consisting of stop_words to ignore\n",
    "\n",
    "@behavior preprocesses the sentence\n",
    "\n",
    "@output a triple consisting of the length in characters of the sentence, the number of terms in the sentence and\n",
    "a list of terms and noun phrases appearing in the sentence (with repeated terms) \n",
    "'''\n",
    "def preprocess(sentence: str, tokenizer: nltk.tokenize.api.TokenizerI, wnl: WordNetLemmatizer, stop_words=set):\n",
    "    sent_out = list()\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
    "    for term in tokenized_sentence:\n",
    "        lem_term = wnl.lemmatize(term)\n",
    "        if lem_term not in stop_words:      \n",
    "            sent_out.append(lem_term)\n",
    "    blobbed_sentence = TextBlob(sentence)\n",
    "    all_noun_phrases = blobbed_sentence.noun_phrases\n",
    "    # only include those that have multiple words\n",
    "    noun_phrases = [n_p for n_p in all_noun_phrases if ' ' in n_p]\n",
    "    sent_out.extend(noun_phrases)\n",
    "    return len(sentence), len(tokenized_sentence), sent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "indexing(D,args)\n",
    "    @input document collection D and optional arguments on text preprocessing\n",
    "\n",
    "    @behavior preprocesses the collection and, using existing libraries, \n",
    "    builds an inverted index with the relevant statistics for the subsequent summarization functions\n",
    "    \n",
    "    @output pair with the inverted index I and indexing time\n",
    "'''\n",
    "def indexing(articles, **args) -> InvertedIndex:\n",
    "    start_time = time.time()\n",
    "    inverted_index = InvertedIndex(len(articles))\n",
    "\n",
    "    # tokenizer split words and keep hyphens e.g. state-of-the-art\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|-]+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # loop through collection \n",
    "    for article_id, article in enumerate(articles): \n",
    "        sents = sentence_tokenize(article)\n",
    "        # remove title (not needed for summarization task)\n",
    "        sents = sents[1:]\n",
    "        preprocessing_results = [preprocess(sent, tokenizer, wnl, stop_words) for sent in sents]\n",
    "        sent_num_chars, sent_term_counts, preprocessed_sentences = zip(*preprocessing_results)\n",
    "        inverted_index.sentence_num_chars.append(list(sent_num_chars))\n",
    "        inverted_index.sentence_term_counts.append(list(sent_term_counts))\n",
    "        # count the term frequencies per sentence\n",
    "        term_counter_per_sent = [Counter(sentence_terms) for sentence_terms in preprocessed_sentences]\n",
    "        for sent_number, term_counter in enumerate(term_counter_per_sent):\n",
    "            for term in term_counter: \n",
    "                tf = term_counter[term]\n",
    "                term_document_tfs = inverted_index.get_or_default(term, article_id)\n",
    "                term_document_tfs.tf_d_t += tf \n",
    "                term_document_tfs.add_sentence(sent_number, tf)\n",
    "                inverted_index.update(term, article_id, term_document_tfs)\n",
    "    inverted_index.calculate_dfs()\n",
    "    end_time = time.time()\n",
    "    indexing_time = end_time - start_time\n",
    "    inverted_index.set_indexing_time(indexing_time)\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize(\"played\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [str(i) for i in range(200000)]\n",
    "sws = set(stopwords.words('english'))\n",
    "[term in sws for term in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_removed = 'Hello, I\\'m a data scientist and machine learning enthusiast. My name is John Williams and am a data scientist. The majestic, centuries-old oak tree stood proudly at the edge of the meadow'\n",
    "blob = TextBlob(sent_removed)\n",
    "[a for a in blob.noun_phrases if ' ' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'Title. The little white little rabbit. The person played with the ball.'\n",
    "s1 = 'Title. The white rabbit\\'s ball. Rabbit rabbit ball rabbit.'\n",
    "s2 = 'Title.  White, the little white rabbit. Little, little.'\n",
    "test = [s0, s1, s2]\n",
    "I_test = indexing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I_test.doc_to_string(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = indexing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I.sentence_term_counts[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\", \"business\", \"509.txt\")\n",
    "\n",
    "print(I.doc_to_string(map_path_to_articleID(document_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization \n",
    "\n",
    "TF: \n",
    "* Document: Term frequencies are assessed on document level.\n",
    "* Sentence: Term frequencies are assessed on sentence level.\n",
    "\n",
    "IDF: Inverted document frequencies is assessed on collection level.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "TODO: \n",
    "* Evaluate choice and give reason: \n",
    "    * IDF on document level?\n",
    "    * TF on document level for sentences? \n",
    "* \"order\" parameter o\n",
    "* BM25\n",
    "* BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10_tf(x):\n",
    "    try:\n",
    "        return (1 + math.log10(x)) \n",
    "    except ValueError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_term(N, df_t, tf_t_d):\n",
    "    return log10_tf(tf_t_d) * math.log10(N/df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_term(df_t, tf_t_d, N, s_len_avg, s_len, k, b): \n",
    "    idf_t = math.log10(N/df_t)\n",
    "    B = 1 - b + b * (s_len/s_len_avg)\n",
    "    return idf_t * (tf_t_d * (k + 1))/(tf_t_d + k * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(d: dict, max_elements: int, reverse=False) -> dict: \n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=reverse)[:max_elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_by_value(d: dict) -> tuple:\n",
    "    return max(d.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_key(d: dict) -> dict: \n",
    "    return dict(sorted(d.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_order(scores: dict, o: str, p: int, l: int, sentence_lengths: list): \n",
    "    # Don't exceed maximum number of sentences. If it doesn't matter it should be set to 0\n",
    "    max_elements = p if p else len(scores)\n",
    "    sorted_scores = sort_by_value(scores, max_elements=max_elements, reverse=True)\n",
    "\n",
    "    # Don't exceed maximum number of characters. If it doesn't matter it should be set to 0\n",
    "    if l:\n",
    "        total_length = 0\n",
    "        cropped_sorted_scores = dict()\n",
    "        for sent_number, score in sorted_scores.items():\n",
    "            sent_length = sentence_lengths[sent_number]\n",
    "            total_length += sent_length\n",
    "            if total_length > l:\n",
    "                break\n",
    "            cropped_sorted_scores[sent_number] = score\n",
    "        sorted_scores = cropped_sorted_scores\n",
    "\n",
    "    if o == \"rel\": \n",
    "        return sorted_scores\n",
    "    elif o == \"app\": \n",
    "        return sort_by_key(sorted_scores)\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence: str, model, tokenizer, device, max_length=512) -> torch.tensor: \n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    encoded_input.to(device)\n",
    "    output = model(**encoded_input)\n",
    "    embedding = output[\"pooler_output\"].squeeze()\n",
    "    # mean pooled embedding might be better\n",
    "    # mean_pooled_embedding = last_hidden_states.mean(axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_relevance_scores(info: list, N):\n",
    "    scores = defaultdict(int)\n",
    "    sq_normalization_term = defaultdict(int)\n",
    "    for _, df_t, tf_t_d, tf_per_sentence in info:\n",
    "        rel_t_d = tf_idf_term(N, df_t, tf_t_d)\n",
    "        for sent_number, tf_s_t in tf_per_sentence:\n",
    "            scores[sent_number] += rel_t_d * log10_tf(tf_s_t)\n",
    "            sq_normalization_term[sent_number] = log10_tf(tf_s_t)**2\n",
    "    # normalization\n",
    "    for sent_number, score in scores.items():\n",
    "        scores[sent_number] = score / math.sqrt(sq_normalization_term[sent_number])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_sent_similarity():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_info_after_sent_removal(info: list, sent_removed: int):\n",
    "   for i, (term, df_t, tf_t_d, tf_per_sentence) in enumerate(info):\n",
    "      info[i][2] -= tf_per_sentence[sent_removed]  \n",
    "      info[i][3] = [(sent_number, score) for sent_number, score in info[i][3] if sent_number != sent_removed]\n",
    "      if info[i][2] == 0:\n",
    "         info[i][1] -= 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summarization(d,p,l,o,I,args)\n",
    "    @input document d (the index in I/D), maximum number of sentences (p) and/or characters (l), order\n",
    "    of presentation o (appearance in text vs relevance), inverted index I or the\n",
    "    collection D, and optional arguments on IR models\n",
    "\n",
    "    @behavior preprocesses d, assesses the relevance of each sentence in d against I ac-\n",
    "    cording to args, and presents them in accordance with p, l and o\n",
    "    \n",
    "    @output summary s of document d, i.e. ordered pairs (sentence position in d, score)\n",
    "'''\n",
    "def summarization(d: int, p: int, l: int, o: int, I_or_D: Union[InvertedIndex, list], **args) -> list:\n",
    "\n",
    "    ## if we receive the collection instead of the inverted index we must compute it first\n",
    "    if type(I_or_D) == list:\n",
    "        I = indexing(I_or_D)         \n",
    "    else: \n",
    "        I = I_or_D\n",
    "        \n",
    "    doc_info = I.get_document_info(d)\n",
    "    term_doc_info = zip(*doc_info.values())      \n",
    "    sentence_lengths = I.get_sentence_lengths(d)\n",
    "    scores = defaultdict(int)\n",
    "    sq_normalization_term = defaultdict(int)\n",
    "\n",
    "    if args['model'] == 'TF-IDF':\n",
    "        scores = tf_idf_relevance_scores(term_doc_info, I.N)\n",
    "    \n",
    "    elif args['model'] == 'BM25':\n",
    "        k = 0.2\n",
    "        b = 0.75 \n",
    "        avg_sentence_length = sum(sentence_lengths)/len(sentence_lengths)\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info: \n",
    "            for sent_number, tf_s_t in tf_per_sentence: \n",
    "                scores[sent_number] += BM25_term(df_t, tf_s_t, I.N, avg_sentence_length, sentence_lengths[sent_number], k, b)\n",
    "    \n",
    "    elif args['model'] == 'BERT':\n",
    "        document = I_or_D[d]\n",
    "        \n",
    "        tokenizer = args['bert_tokenizer']\n",
    "        bert_model = args['bert_model']\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        bert_model.to(device)\n",
    "        \n",
    "        scores = defaultdict(float)\n",
    "        # sentences \n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        sentences = sentences[1:]\n",
    "        num_sentences = len(sentences)\n",
    "        sent_embeddings = list()\n",
    "        # every sentences on its own, no padding needed, faster on cpu\n",
    "        # for gpu batches are better \n",
    "        for sent in sentences: \n",
    "            sent_embedding = get_embedding(sent, bert_model, tokenizer, device)\n",
    "            sent_embeddings.append(sent_embedding)\n",
    "        # document\n",
    "        doc_embedding = get_embedding(document, bert_model, tokenizer, device, max_length=512)\n",
    "        for sent_id in range(0, num_sentences): \n",
    "            sent_vec = sent_embeddings[sent_id]\n",
    "            score = torch.nn.functional.cosine_similarity(doc_embedding, sent_vec, dim=0)\n",
    "            scores[sent_id] = score.item()\n",
    "    \n",
    "    elif args['model'] == 'MMR-TFIDF1':\n",
    "        runs = p or len(scores)\n",
    "        available_sentences = dict(scores)\n",
    "        selected_sentences = defaultdict(int)\n",
    "        for _ in range(runs):\n",
    "            relevance_scores = tf_idf_relevance_scores(term_doc_info, I.N)\n",
    "            sent_similarity = tf_idf_sent_similarity()\n",
    "            best_sentence = max_by_value(scores)\n",
    "        \n",
    "    \n",
    "    elif args['model'] == 'MMR-TFIDF2':\n",
    "        runs = p or len(scores)\n",
    "        available_sentences = dict(scores)\n",
    "        selected_sentences = defaultdict(int)\n",
    "        for _ in range(runs):\n",
    "            relevance_scores = tf_idf_relevance_scores(term_doc_info, I.N)\n",
    "            sent_similarity = tf_idf_sent_similarity()\n",
    "            best_sentence = max_by_value(scores)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Currently we only support the following models:\\n→ TF-IDF\\n→ BM-25\\n→ BERT\\n→MMR-TFIDF\")\n",
    "    \n",
    "    return sort_by_order(scores, o, p, l, sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=7, l=1000, o=\"app\", I_or_D=I, model='TF-IDF')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=I, model='BM25')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=articles, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "for path in article_file_paths: \n",
    "    article_id = map_path_to_articleID(path)\n",
    "    article_score = summarization(d=article_id, p=8, l=1000, o=\"app\", I_or_D=I, model='TF-IDF')\n",
    "    scores.append(article_score)\n",
    "with open('./testing/extracts/tf_idf/scores.json', 'w') as f: \n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "Calculates the keywords based on the tf-idf of the document.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "Parameter for including only noun phrases. \n",
    "\n",
    "No need of BERT (see assigment sheet, p.4 IR Models)\n",
    "\n",
    "should be primarly based on TF-IDF\n",
    "\n",
    "\n",
    "TODO:\n",
    "* Nouns: just unigrams or also bigrams?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.classify import Senna\n",
    "from nltk.tag import SennaChunkTagger\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "keyword extraction(d,p,I,args)\n",
    "    @input document d, maximum number of keywords p, inverted index I, and op-\n",
    "    tional arguments on IR model choices\n",
    "\n",
    "    @behavior extracts the top informative p keywords in d against I according to args\n",
    "    \n",
    "    @output ordered set of p keywords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(d: int, p: int, I: InvertedIndex, **args) -> dict:\n",
    "     \n",
    "    doc_info = I.get_document_info(d)\n",
    "    term_doc_info = zip(*doc_info.values())    \n",
    "\n",
    "    scores = defaultdict(str)\n",
    "\n",
    "    for term, df_t, tf_t_d, tf_per_sentence in term_doc_info:\n",
    "        rel_t_d = tf_idf_term(I.N, df_t, tf_t_d)\n",
    "        scores[term] = rel_t_d\n",
    "    scores = sort_by_value(scores, p, reverse=True)\n",
    "    return scores         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "scores = keyword_extraction(article_id, 10, I)\n",
    "print(scores)\n",
    "\n",
    "print(I.doc_to_string(article_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "TODO:\n",
    "* Implement evaluation\n",
    "* Evaluation:\n",
    "    * Statistics \n",
    "    * F-meassure\n",
    "    * Recall-precision-curve\n",
    "    * MAP\n",
    "    * Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "evaluation(Sset,Rset,args)\n",
    "    @input the set of summaries Sset produced from selected documents Dset ⊆ D\n",
    "    (e.g. a single document, a category of documents, the whole collection),\n",
    "    the corresponding reference extracts Rset, and optional arguments (evalu-\n",
    "    ation, preprocessing, model options)\n",
    "\n",
    "    @behavior assesses the produced summaries against the reference ones using the tar-\n",
    "    get evaluation criteria\n",
    "\n",
    "    @output evaluation statistics, including F-measuring at predefined p-or-l summary\n",
    "    limits, recall-and-precision curves, MAP, and efficiency\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    return len(relevant_indices.intersection(extracted_indices))/len(extracted_indices)\n",
    "\n",
    "def get_recall(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    return len(relevant_indices.intersection(extracted_indices))/len(relevant_indices)\n",
    "\n",
    "def get_F1(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    p = get_precision(extracted_indices, relevant_indices)\n",
    "    r = get_recall(extracted_indices, relevant_indices)\n",
    "    if p+r == 0: return 0\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(S: list, R: list, **args) -> list:\n",
    "    precisions = list()\n",
    "    recalls = list() \n",
    "    f1_scores = list()\n",
    "    for extracted_indices_rel, relevant_indices in zip(R, S):  \n",
    "        extracted_indices = list(extracted_indices_rel.keys())\n",
    "        extracted_indices = list(map(int, extracted_indices))\n",
    "        precisions.append(get_precision(extracted_indices, relevant_indices))\n",
    "        recalls.append(get_recall(extracted_indices, relevant_indices))\n",
    "        f1_scores.append(get_F1(extracted_indices, relevant_indices))\n",
    "    mean_precision = sum(precisions)/len(precisions)\n",
    "    mean_recall = sum(recalls)/len(recalls)\n",
    "    mean_f1_scores = sum(f1_scores)/len(f1_scores)\n",
    "\n",
    "    # precision-recall-curve\n",
    "    recalls, precisions = (list(t) for t in zip(*sorted(zip(recalls, precisions))))\n",
    "    plt.plot(recalls, precisions)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "    #precision_recall_curve = precision_recall_curve(precisions, recalls)\n",
    "    #display = PrecisionRecallDisplay(precisions, recalls)\n",
    "    #display.plot()\n",
    "    \n",
    "    # MAP - mean average Precision? \n",
    "    # average precision per class - category?\n",
    "    \n",
    "    return mean_precision, mean_recall, mean_f1_scores, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing/reference/summary_sentence_indices.json\", \"r\") as f:\n",
    "    summary_sentence_indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing/extracts/tf_idf/scores.json\", \"r\") as f: \n",
    "    tf_idf_extract = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(summary_sentence_indices, tf_idf_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset for BERT comparison\\\n",
    "takes too long otherwise\\ \n",
    "\n",
    "TODO: \n",
    "* evaluation + first question -> Sebastian   \n",
    "* question 2, 3, 4 -> Francisco\n",
    "* question 5 and 6 -> Tuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "summarization(articles[article_id], num_sent=5, order=\"rel\", inverted_index=I, N=len(articles), article_id=article_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RRF(article_id, index, collection, mu=5, p=5):\n",
    "    \n",
    "    #get all scores for BERT\n",
    "    scores_bert = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=collection, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "    ranks_bert = {key: rank for rank, key in enumerate(scores_bert, 1)}\n",
    "    #get all scores of BM25\n",
    "    scores_bm25 = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=index, model='BM25')\n",
    "    ranks_bm25 = {key: rank for rank, key in enumerate(scores_bm25, 1)} \n",
    "    \n",
    "    #get RRF\n",
    "    scores_rrf = defaultdict(int)\n",
    "    for sent_number in ranks_bert.keys():\n",
    "        scores_rrf[sent_number] = 1/(mu + ranks_bert[sent_number]) + 1/(mu + ranks_bm25[sent_number])\n",
    "        \n",
    "    return sort_by_value(scores_rrf, p, reverse=True)\n",
    "\n",
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = get_RRF (article_id, I, articles, mu=5, p=5)\n",
    "scores = dict(sorted(scores.items()))\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, rank in scores.items(): \n",
    "    print(f\"{rank:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMR \n",
    "Maximal Marginal Relevance (MRR) is a method that can be used to increase sentence diversity. \n",
    "\n",
    "The proposed method iteratively selects the most informative sentence according to a given IR model, adding it to the set of sentences in the summary and removing it from the document. Next sentence selection is based on the MMR score that simultaneously attempts to select sentences that are relevant and dissimilar to the sentences selected so far (non-redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "keyword_extraction(articles[0], 10, I, len(articles), 0, use_only_nouns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_sentences(documents: list) -> list: \n",
    "    number_of_sentences = list()\n",
    "    for document in documents: \n",
    "        sents = sent_tokenize(document) \n",
    "        number_of_sentences.append(len(sents))\n",
    "    return number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(documents: list) -> Union[float, float, float]: \n",
    "    number_of_sentences = get_number_of_sentences(documents)\n",
    "    mean = statistics.mean(number_of_sentences)\n",
    "    median = statistics.median(number_of_sentences)\n",
    "    std = statistics.stdev(number_of_sentences)\n",
    "    return mean, std, median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(documents: list):\n",
    "    number_of_sentences = get_number_of_sentences(documents)\n",
    "    mean, std, median = get_statistics(documents)\n",
    "    plt.hist(number_of_sentences, bins=80)\n",
    "    plt.title(f\"mean: {mean:.2f}, std: {std:.2f}, median: {median:.2f}\")\n",
    "    plt.xlabel(\"Number of sentences\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "\n",
    "# average number of sentences \n",
    "mean, std, median = get_statistics(articles)\n",
    "plot_distribution(articles)\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")\n",
    "print(f\"Median: {median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std, median = get_statistics(summaries)\n",
    "plot_distribution(summaries)\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")\n",
    "print(f\"Median: {median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = [len(l) for l in summary_sentence_indices]\n",
    "mean = statistics.mean(number_of_sentences)\n",
    "median = statistics.median(number_of_sentences)\n",
    "std = statistics.stdev(number_of_sentences)\n",
    "plt.hist(number_of_sentences, bins=80)\n",
    "plt.title(f\"mean: {mean:.2f}, std: {std:.2f}, median: {median:.2f}\")\n",
    "plt.xlabel(\"Number of sentences\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
