{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 11**\n",
    "- Francisco Martins, 99068\n",
    "- Tunahan Güneş, 108108\n",
    "- Sebastian Weidinger, 111612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    texts = []\n",
    "    file_paths = []\n",
    "    for folder in os.listdir(path):\n",
    "        category_path = os.path.join(path, folder)\n",
    "        texts.append([])\n",
    "        for file in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, file)\n",
    "            file_paths.append(file_path)\n",
    "            with open(file_path, \"r\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                texts[-1].append(text)\n",
    "                \n",
    "    print(\"Number of Categories:\",len(os.listdir(path)))\n",
    "    for i in range(len(os.listdir(path))):\n",
    "        print(\"Number of Articles in\", \"'\"+os.listdir(path)[i]+\"'\", \"Category:\",len(texts[i]))\n",
    "    return file_paths, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "file_paths, categorized_articles = read_files(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examplary text. The structure of the read file is: articles[category_no][document_no]. \n",
    "print(categorized_articles[0][0])\n",
    "print(file_paths[508:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from typing import Union\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter, defaultdict\n",
    "from tabulate import tabulate\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list to get uncategorized collection \n",
    "def flatten(lists) -> list: \n",
    "    return [element for sublist in lists for element in sublist]\n",
    "\n",
    "articles = flatten(categorized_articles)\n",
    "N = len(articles)\n",
    "dict_path_to_articleID = {path:i for i, path in enumerate(file_paths)}\n",
    "\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index Structure \n",
    " \n",
    "Each term points to a dictionary of document identifier and the term frequency in the document.\n",
    "\n",
    "t1 -> {doc1: TF, doc5: TF, ...}\\\n",
    "t2 -> {doc7: TF, doc8: TF, ...}\\\n",
    "...\n",
    "t2 -> [DF, {doc7: [TF_(t2, doc7), {s1: TF, s4: TF, ...}], doc8: [TF_(t2, doc8), {s2: TF, s4: TF, ...}], ...}]\\\n",
    "\n",
    "use class structure\n",
    "\n",
    "TODO: \n",
    "* Optimize structure?\n",
    "    * Is there a more efficient way? \n",
    "    * Add maybe pointers to sentences and their term frequency? -> Faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermFrequencies: \n",
    "    def __init__(self) -> None:\n",
    "        self.tf_d_t = 0\n",
    "        self.sent_tf = list()\n",
    "\n",
    "    def add_sentence(self, sent_number, term_frequency):\n",
    "        self.sent_tf.append((sent_number, term_frequency))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        padding = 5 - len(str(self.tf_d_t))\n",
    "        return f'TF_d_t: {self.tf_d_t}{\" \" * padding}TF_per_sentence: {self.sent_tf}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexEntry:\n",
    "    def __init__(self) -> None:\n",
    "        self.df_term = 0\n",
    "        self.term_dict = defaultdict(TermFrequencies)\n",
    "    \n",
    "    def get_document(self, document):\n",
    "        return self.term_dict.get(document, None)\n",
    "\n",
    "    def get_or_default_document(self, document):\n",
    "        return self.term_dict[document]\n",
    "\n",
    "    def update_document(self, document, new_value):\n",
    "        self.term_dict[document] = new_value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Document Frequency: {self.df_term}\\n {\" \" * (max_width+2)} Term frequencies:\\n'\n",
    "        for doc_number, tfs in self.term_dict.items():\n",
    "            padding = 5 - len(str(doc_number))\n",
    "            out += f'{\" \" * (max_width + 3)} Doc {doc_number}{\" \" * padding}→ {tfs}\\n'\n",
    "        return out\n",
    "    \n",
    "    def calculate_df(self):\n",
    "        self.df_term = len(self.term_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, collection_size) -> None:\n",
    "        self.inverted_index = defaultdict(InvertedIndexEntry)\n",
    "        self.sentence_term_counts = list()\n",
    "        self.sentence_num_chars = list()\n",
    "        self.indexing_time = 0\n",
    "        self.N = collection_size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Time to index: {self.indexing_time}\\nInverted Index:\\n'\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            padding = max_width - len(term)\n",
    "            out += f'{term} {\" \" * padding} → {entry}\\n'\n",
    "        return out\n",
    "\n",
    "    def get_or_default(self, term, document):\n",
    "        return self.inverted_index[term].get_or_default_document(document)\n",
    "    \n",
    "    def update(self, term, document, new_value):\n",
    "        self.inverted_index[term].update_document(document, new_value)\n",
    "    \n",
    "    def set_indexing_time(self, indexing_time):\n",
    "        self.indexing_time = indexing_time\n",
    "    \n",
    "    def calculate_dfs(self):\n",
    "        for entry in self.inverted_index.values():\n",
    "            entry.calculate_df()  \n",
    "    \n",
    "    def get_sentence_lengths(self, document):\n",
    "        return self.sentence_term_counts[document]\n",
    "\n",
    "    def get_document_info(self, document):          \n",
    "        info = {'Vocabulary': [], 'DF_t': [], 'TF_d_t': [], 'TF/sentence': []}\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            doc_tfs = entry.get_document(document)\n",
    "            if doc_tfs == None:\n",
    "                continue\n",
    "            info['Vocabulary'].append(term)\n",
    "            info['DF_t'].append(entry.df_term)\n",
    "            info['TF_d_t'].append(doc_tfs.tf_d_t)\n",
    "            info['TF/sentence'].append(doc_tfs.sent_tf)\n",
    "        return info\n",
    "    \n",
    "    def doc_to_string(self, document: int):\n",
    "        out = f'Document id={document} → vocabulary and term frequencies:\\n'\n",
    "        info = self.get_document_info(document)\n",
    "        table = zip(*info.values())\n",
    "        headers = info.keys()\n",
    "        return out + tabulate(table, headers, tablefmt=\"pretty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(sentence):\n",
    "    sents = list()\n",
    "    for paragraph in sentence.split('\\n '):\n",
    "        # split into sentences  \n",
    "        sents_p = nltk.sent_tokenize(paragraph)\n",
    "        for sent in sents_p:\n",
    "            sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@ input a sentence to process, a tokenizer to split it into terms, a lemmatizer to normalize the terms,\n",
    "a set consisting of stop_words to ignore\n",
    "\n",
    "@behavior preprocesses the sentence\n",
    "\n",
    "@output a triple consisting of the length in characters of the sentence, the number of terms in the sentence and\n",
    "a list of terms and noun phrases appearing in the sentence (with repeated terms) \n",
    "'''\n",
    "def preprocess(sentence: str, tokenizer: nltk.tokenize.api.TokenizerI, wnl: WordNetLemmatizer, stop_words=set):\n",
    "    sent_out = list()\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
    "    for term in tokenized_sentence:\n",
    "        lem_term = wnl.lemmatize(term)\n",
    "        if lem_term not in stop_words:      \n",
    "            sent_out.append(lem_term)\n",
    "    blobbed_sentence = TextBlob(sentence)\n",
    "    all_noun_phrases = blobbed_sentence.noun_phrases\n",
    "    # only include those that have multiple words\n",
    "    noun_phrases = [n_p for n_p in all_noun_phrases if ' ' in n_p]\n",
    "    sent_out.extend(noun_phrases)\n",
    "    return len(sentence), len(tokenized_sentence), sent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "indexing(D,args)\n",
    "    @input document collection D and optional arguments on text preprocessing\n",
    "\n",
    "    @behavior preprocesses the collection and, using existing libraries, \n",
    "    builds an inverted index with the relevant statistics for the subsequent summarization functions\n",
    "    \n",
    "    @output pair with the inverted index I and indexing time\n",
    "'''\n",
    "def indexing(articles, **args) -> InvertedIndex:\n",
    "    start_time = time.time()\n",
    "    inverted_index = InvertedIndex(len(articles))\n",
    "\n",
    "    # tokenizer split words and keep hyphens e.g. state-of-the-art\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|-]+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # loop through collection \n",
    "    for article_id, article in enumerate(articles): \n",
    "        sents = sentence_tokenize(article)\n",
    "        # remove title (not needed for summarization task)\n",
    "        sents = sents[1:]\n",
    "        preprocessing_results = [preprocess(sent, tokenizer, wnl, stop_words) for sent in sents]\n",
    "        sent_num_chars, sent_term_counts, preprocessed_sentences = zip(*preprocessing_results)\n",
    "        inverted_index.sentence_num_chars.append(list(sent_num_chars))\n",
    "        inverted_index.sentence_term_counts.append(list(sent_term_counts))\n",
    "        # count the term frequencies per sentence\n",
    "        term_counter_per_sent = [Counter(sentence_terms) for sentence_terms in preprocessed_sentences]\n",
    "        for sent_number, term_counter in enumerate(term_counter_per_sent):\n",
    "            for term in term_counter: \n",
    "                tf = term_counter[term]\n",
    "                term_document_tfs = inverted_index.get_or_default(term, article_id)\n",
    "                term_document_tfs.tf_d_t += tf \n",
    "                term_document_tfs.add_sentence(sent_number, tf)\n",
    "                inverted_index.update(term, article_id, term_document_tfs)\n",
    "    inverted_index.calculate_dfs()\n",
    "    end_time = time.time()\n",
    "    indexing_time = end_time - start_time\n",
    "    inverted_index.set_indexing_time(indexing_time)\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize(\"played\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [str(i) for i in range(200000)]\n",
    "sws = set(stopwords.words('english'))\n",
    "[term in sws for term in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data scientist',\n",
       " 'machine learning enthusiast',\n",
       " 'john williams',\n",
       " 'data scientist',\n",
       " 'centuries-old oak tree']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Hello, I\\'m a data scientist and machine learning enthusiast. My name is John Williams and am a data scientist. The majestic, centuries-old oak tree stood proudly at the edge of the meadow'\n",
    "blob = TextBlob(s)\n",
    "[a for a in blob.noun_phrases if ' ' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'Title. The little white little rabbit. The person played with the ball.'\n",
    "s1 = 'Title. The white rabbit\\'s ball. Rabbit rabbit ball rabbit.'\n",
    "s2 = 'Title.  White, the little white rabbit. Little, little.'\n",
    "test = [s0, s1, s2]\n",
    "I_test = indexing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to index: 0.011164665222167969\n",
      "Inverted Index:\n",
      "little                → Document Frequency: 2\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 2    TF_per_sentence: [(0, 2)]\n",
      "                        Doc 2    → TF_d_t: 3    TF_per_sentence: [(0, 1), (1, 2)]\n",
      "\n",
      "white                 → Document Frequency: 3\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "                        Doc 1    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "                        Doc 2    → TF_d_t: 2    TF_per_sentence: [(0, 2)]\n",
      "\n",
      "rabbit                → Document Frequency: 3\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "                        Doc 1    → TF_d_t: 4    TF_per_sentence: [(0, 1), (1, 3)]\n",
      "                        Doc 2    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "\n",
      "white little rabbit   → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "\n",
      "person                → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(1, 1)]\n",
      "\n",
      "played                → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(1, 1)]\n",
      "\n",
      "ball                  → Document Frequency: 2\n",
      "                        Term frequencies:\n",
      "                        Doc 0    → TF_d_t: 1    TF_per_sentence: [(1, 1)]\n",
      "                        Doc 1    → TF_d_t: 2    TF_per_sentence: [(0, 1), (1, 1)]\n",
      "\n",
      "white rabbit 's ball  → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 1    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "\n",
      "rabbit ball rabbit    → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 1    → TF_d_t: 1    TF_per_sentence: [(1, 1)]\n",
      "\n",
      "white rabbit          → Document Frequency: 1\n",
      "                        Term frequencies:\n",
      "                        Doc 2    → TF_d_t: 1    TF_per_sentence: [(0, 1)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(I_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id=2 → vocabulary and term frequencies:\n",
      "+--------------+------+--------+------------------+\n",
      "|  Vocabulary  | DF_t | TF_d_t |   TF/sentence    |\n",
      "+--------------+------+--------+------------------+\n",
      "|    little    |  2   |   3    | [(0, 1), (1, 2)] |\n",
      "|    white     |  3   |   2    |     [(0, 2)]     |\n",
      "|    rabbit    |  3   |   1    |     [(0, 1)]     |\n",
      "| white rabbit |  1   |   1    |     [(0, 1)]     |\n",
      "+--------------+------+--------+------------------+\n"
     ]
    }
   ],
   "source": [
    "print(I_test.doc_to_string(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = indexing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I.sentence_term_counts[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id=508 → vocabulary and term frequencies:\n",
      "+---------------------------+------+--------+----------------------------------------------------+\n",
      "|        Vocabulary         | DF_t | TF_d_t |                    TF/sentence                     |\n",
      "+---------------------------+------+--------+----------------------------------------------------+\n",
      "|           firm            | 434  |   1    |                      [(1, 1)]                      |\n",
      "|            one            | 1010 |   5    |             [(1, 1), (8, 1), (10, 3)]              |\n",
      "|          biggest          | 205  |   2    |                  [(0, 1), (8, 1)]                  |\n",
      "|           said            | 1880 |   2    |                 [(9, 1), (12, 1)]                  |\n",
      "|            11             | 185  |   1    |                      [(7, 1)]                      |\n",
      "|           time            | 854  |   1    |                      [(4, 1)]                      |\n",
      "|         business          | 296  |   4    |         [(2, 1), (3, 1), (7, 1), (10, 1)]          |\n",
      "|            ha             | 1615 |   2    |                 [(8, 1), (10, 1)]                  |\n",
      "|           three           | 528  |   1    |                     [(13, 1)]                      |\n",
      "|          company          | 528  |   6    | [(4, 1), (5, 1), (8, 1), (9, 1), (10, 1), (12, 1)] |\n",
      "|          service          | 363  |   1    |                      [(9, 1)]                      |\n",
      "|         customer          | 117  |   1    |                      [(9, 1)]                      |\n",
      "|           also            | 1264 |   1    |                      [(9, 1)]                      |\n",
      "|          result           | 274  |   1    |                      [(8, 1)]                      |\n",
      "|             u             | 767  |   1    |                     [(11, 1)]                      |\n",
      "|         exchange          |  99  |   1    |                     [(12, 1)]                      |\n",
      "|         slightly          |  58  |   1    |                      [(8, 1)]                      |\n",
      "|            wa             | 1747 |   1    |                     [(11, 1)]                      |\n",
      "|           chief           | 332  |   4    |         [(6, 1), (9, 1), (12, 1), (13, 1)]         |\n",
      "|         executive         | 252  |   4    |         [(6, 1), (9, 1), (12, 1), (13, 1)]         |\n",
      "|          growth           | 225  |   1    |                      [(1, 1)]                      |\n",
      "|          around           | 303  |   1    |                      [(6, 1)]                      |\n",
      "|           part            | 442  |   1    |                      [(7, 1)]                      |\n",
      "|          market           | 376  |   1    |                      [(9, 1)]                      |\n",
      "|          already          | 372  |   1    |                      [(7, 1)]                      |\n",
      "|           deal            | 255  |   1    |                      [(6, 1)]                      |\n",
      "|          europe           | 240  |   2    |                  [(1, 1), (5, 1)]                  |\n",
      "|            new            | 896  |   2    |                 [(5, 1), (12, 1)]                  |\n",
      "|          dollar           | 122  |   1    |                     [(11, 1)]                      |\n",
      "|             1             | 443  |   2    |                 [(12, 1), (13, 1)]                 |\n",
      "|           euro            | 100  |   1    |                     [(12, 1)]                      |\n",
      "|           month           | 540  |   1    |                     [(13, 1)]                      |\n",
      "|            mr             | 796  |   1    |                     [(12, 1)]                      |\n",
      "|           much            | 440  |   1    |                      [(4, 1)]                      |\n",
      "|           year            | 1284 |   4    |         [(1, 1), (2, 1), (8, 1), (11, 1)]          |\n",
      "|           worry           |  59  |   2    |                  [(0, 1), (8, 1)]                  |\n",
      "|           price           | 231  |   1    |                      [(0, 1)]                      |\n",
      "|          fallen           |  43  |   1    |                      [(8, 1)]                      |\n",
      "|           rate            | 178  |   1    |                     [(12, 1)]                      |\n",
      "|             -             | 1071 |   7    |         [(1, 2), (4, 2), (10, 2), (13, 1)]         |\n",
      "|           move            | 305  |   1    |                      [(9, 1)]                      |\n",
      "|           many            | 524  |   1    |                      [(6, 1)]                      |\n",
      "|           could           | 838  |   1    |                      [(9, 1)]                      |\n",
      "|            say            | 596  |   2    |                  [(3, 1), (7, 1)]                  |\n",
      "|           money           | 270  |   1    |                      [(4, 1)]                      |\n",
      "|           2004            | 370  |   1    |                     [(13, 1)]                      |\n",
      "|           cost            | 280  |   1    |                      [(4, 1)]                      |\n",
      "|           last            | 880  |   2    |                 [(8, 1), (13, 1)]                  |\n",
      "|          decline          |  56  |   1    |                     [(11, 1)]                      |\n",
      "|        competition        | 146  |   2    |                  [(0, 1), (8, 1)]                  |\n",
      "|         low-cost          |  16  |   3    |              [(0, 1), (6, 1), (8, 1)]              |\n",
      "|          number           | 533  |   1    |                      [(9, 1)]                      |\n",
      "|          united           | 158  |   1    |                      [(4, 1)]                      |\n",
      "|           state           | 254  |   1    |                      [(4, 1)]                      |\n",
      "|          example          | 111  |   1    |                     [(10, 1)]                      |\n",
      "|           world           | 626  |   2    |                 [(6, 1), (13, 1)]                  |\n",
      "|         suggested         |  97  |   1    |                     [(12, 1)]                      |\n",
      "|          global           | 146  |   4    |             [(3, 1), (9, 1), (10, 2)]              |\n",
      "|          leader           | 232  |   2    |                 [(3, 1), (10, 1)]                  |\n",
      "|            buy            | 130  |   1    |                     [(12, 1)]                      |\n",
      "|           home            | 390  |   1    |                      [(9, 1)]                      |\n",
      "|            two            | 780  |   1    |                      [(2, 1)]                      |\n",
      "|         suggests          |  61  |   1    |                     [(11, 1)]                      |\n",
      "|         according         | 280  |   1    |                     [(10, 1)]                      |\n",
      "|        favourable         |  9   |   1    |                     [(12, 1)]                      |\n",
      "|          called           | 247  |   1    |                      [(5, 1)]                      |\n",
      "|            uk             | 455  |   1    |                      [(1, 1)]                      |\n",
      "|           brown           | 125  |   1    |                     [(10, 1)]                      |\n",
      "|          country          | 413  |   2    |                  [(7, 1), (9, 1)]                  |\n",
      "|           large           | 132  |   1    |                     [(10, 1)]                      |\n",
      "|            oil            | 106  |   1    |                      [(0, 1)]                      |\n",
      "|          nearly           |  91  |   1    |                      [(2, 1)]                      |\n",
      "|           trend           |  63  |   1    |                     [(10, 1)]                      |\n",
      "|           hurt            |  36  |   1    |                      [(9, 1)]                      |\n",
      "|          region           |  58  |   1    |                     [(10, 1)]                      |\n",
      "|        oil prices         |  27  |   1    |                      [(0, 1)]                      |\n",
      "|            30             | 176  |   2    |                 [(2, 1), (12, 1)]                  |\n",
      "|          decided          | 113  |   1    |                      [(6, 1)]                      |\n",
      "|           boss            |  17  |   2    |                  [(2, 1), (7, 1)]                  |\n",
      "|         currently         | 224  |   1    |                      [(5, 1)]                      |\n",
      "|           moved           |  87  |   1    |                      [(7, 1)]                      |\n",
      "|           plan            | 345  |   1    |                      [(7, 1)]                      |\n",
      "|           seen            | 265  |   1    |                     [(11, 1)]                      |\n",
      "|          another          | 306  |   1    |                      [(7, 1)]                      |\n",
      "|          quality          |  88  |   1    |                      [(9, 1)]                      |\n",
      "|          facing           |  78  |   1    |                      [(3, 1)]                      |\n",
      "|           force           | 126  |   1    |                      [(4, 1)]                      |\n",
      "|           risk            | 115  |   1    |                     [(10, 1)]                      |\n",
      "|           rapid           |  26  |   1    |                     [(11, 1)]                      |\n",
      "|         corporate         |  63  |   1    |                      [(4, 1)]                      |\n",
      "|          growing          | 122  |   1    |                      [(9, 1)]                      |\n",
      "|            28             |  60  |   1    |                      [(7, 1)]                      |\n",
      "|            ago            | 194  |   2    |                 [(2, 1), (11, 1)]                  |\n",
      "|          scandal          |  38  |   1    |                      [(4, 1)]                      |\n",
      "|          future           | 254  |   1    |                      [(7, 1)]                      |\n",
      "|          complex          |  39  |   1    |                      [(5, 1)]                      |\n",
      "|         position          | 151  |   1    |                     [(12, 1)]                      |\n",
      "|         us dollar         |  20  |   1    |                     [(11, 1)]                      |\n",
      "|           huge            | 171  |   1    |                     [(11, 1)]                      |\n",
      "|          survey           |  79  |   4    |         [(2, 1), (7, 1), (11, 1), (13, 1)]         |\n",
      "|           even            | 348  |   1    |                     [(11, 1)]                      |\n",
      "|          calling          |  40  |   1    |                      [(8, 1)]                      |\n",
      "|          problem          | 339  |   1    |                     [(11, 1)]                      |\n",
      "|          process          | 120  |   1    |                      [(5, 1)]                      |\n",
      "|         operation         |  98  |   1    |                     [(10, 1)]                      |\n",
      "|          polled           |  5   |   1    |                      [(7, 1)]                      |\n",
      "|         conducted         |  29  |   1    |                      [(2, 1)]                      |\n",
      "|           work            | 445  |   1    |                      [(9, 1)]                      |\n",
      "|          moving           |  64  |   1    |                      [(5, 1)]                      |\n",
      "|         adjusted          |  12  |   1    |                     [(12, 1)]                      |\n",
      "|           clear           | 184  |   1    |                     [(10, 1)]                      |\n",
      "|        competitor         |  30  |   1    |                      [(6, 1)]                      |\n",
      "|         meanwhile         | 127  |   1    |                      [(5, 1)]                      |\n",
      "|          others           | 157  |   1    |                     [(12, 1)]                      |\n",
      "|           harm            |  24  |   1    |                      [(9, 1)]                      |\n",
      "|         prospect          |  50  |   1    |                      [(2, 1)]                      |\n",
      "|        confidence         |  84  |   1    |                      [(1, 1)]                      |\n",
      "|         standard          | 132  |   1    |                      [(5, 1)]                      |\n",
      "|           stock           |  97  |   1    |                      [(5, 1)]                      |\n",
      "|         concerned         | 104  |   1    |                      [(9, 1)]                      |\n",
      "|       third-largest       |  4   |   1    |                     [(11, 1)]                      |\n",
      "|          cheaper          |  35  |   1    |                      [(9, 1)]                      |\n",
      "|      sarbanes-oxley       |  4   |   1    |                      [(4, 1)]                      |\n",
      "|            act            | 148  |   1    |                      [(4, 1)]                      |\n",
      "|           enron           |  10  |   1    |                      [(4, 1)]                      |\n",
      "|         worldcom          |  15  |   1    |                      [(4, 1)]                      |\n",
      "|    corporate scandals     |  2   |   1    |                      [(4, 1)]                      |\n",
      "|          middle           |  63  |   1    |                     [(10, 1)]                      |\n",
      "|     business leaders      |  7   |   1    |                      [(3, 1)]                      |\n",
      "|          provide          | 120  |   1    |                      [(9, 1)]                      |\n",
      "|          across           | 182  |   1    |                      [(5, 1)]                      |\n",
      "|        significant        |  93  |   1    |                      [(8, 1)]                      |\n",
      "|            red            |  69  |   1    |                      [(6, 1)]                      |\n",
      "|           slow            |  55  |   1    |                      [(1, 1)]                      |\n",
      "|          western          |  29  |   1    |                      [(1, 1)]                      |\n",
      "|          locked           |  16  |   1    |                     [(12, 1)]                      |\n",
      "|           east            |  80  |   1    |                     [(10, 1)]                      |\n",
      "|            lot            | 267  |   1    |                      [(4, 1)]                      |\n",
      "|           cited           |  13  |   1    |                     [(11, 1)]                      |\n",
      "|        accounting         |  24  |   1    |                      [(5, 1)]                      |\n",
      "|        throughout         |  61  |   1    |                     [(13, 1)]                      |\n",
      "|          threat           |  87  |   4    |             [(0, 1), (8, 2), (11, 1)]              |\n",
      "|            ups            |  5   |   1    |                      [(0, 1)]                      |\n",
      "|         paperwork         |  4   |   1    |                      [(4, 1)]                      |\n",
      "|        reputation         |  34  |   1    |                      [(9, 1)]                      |\n",
      "|       successfully        |  20  |   1    |                     [(12, 1)]                      |\n",
      "|          assault          |  20  |   1    |                      [(3, 1)]                      |\n",
      "|          appear           |  80  |   1    |                      [(6, 1)]                      |\n",
      "|          upside           |  3   |   1    |                     [(10, 1)]                      |\n",
      "|        regulatory         |  17  |   1    |                      [(3, 1)]                      |\n",
      "|          adviser          |  25  |   1    |                      [(2, 1)]                      |\n",
      "|          anymore          |  13  |   2    |                 [(10, 1), (11, 1)]                 |\n",
      "|          reality          |  39  |   1    |                     [(12, 1)]                      |\n",
      "|  pricewaterhousecoopers   |  3   |   2    |                 [(2, 1), (13, 1)]                  |\n",
      "|            pwc            |  2   |   3    |             [(2, 1), (9, 1), (10, 1)]              |\n",
      "|          string           |  19  |   1    |                      [(4, 1)]                      |\n",
      "|          wo n't           |  39  |   1    |                     [(10, 1)]                      |\n",
      "|        slow growth        |  2   |   1    |                      [(1, 1)]                      |\n",
      "|           tape            |  16  |   1    |                      [(6, 1)]                      |\n",
      "|         advisory          |  10  |   1    |                     [(10, 1)]                      |\n",
      "|     chief executives      |  4   |   3    |             [(6, 1), (12, 1), (13, 1)]             |\n",
      "|        transparent        |  12  |   1    |                      [(4, 1)]                      |\n",
      "|         interview         |  64  |   1    |                     [(12, 1)]                      |\n",
      "|        interviewed        |  19  |   1    |                     [(13, 1)]                      |\n",
      "|          hardly           |  26  |   1    |                      [(6, 1)]                      |\n",
      "|          terror           |  29  |   1    |                      [(0, 1)]                      |\n",
      "|          avoided          |  19  |   1    |                      [(6, 1)]                      |\n",
      "|            54             |  18  |   1    |                      [(8, 1)]                      |\n",
      "|         exception         |  8   |   1    |                      [(1, 1)]                      |\n",
      "|         possibly          |  26  |   1    |                      [(8, 1)]                      |\n",
      "|          gloomy           |  6   |   1    |                      [(2, 1)]                      |\n",
      "|          unlike           |  25  |   1    |                     [(11, 1)]                      |\n",
      "|          samuel           |  8   |   1    |                      [(9, 1)]                      |\n",
      "|           frank           |  39  |   1    |                     [(10, 1)]                      |\n",
      "|       surprisingly        |  9   |   1    |                     [(11, 1)]                      |\n",
      "|      over-regulation      |  1   |   1    |                      [(0, 1)]                      |\n",
      "|           wild            |  26  |   1    |                      [(0, 1)]                      |\n",
      "|      terror threats       |  2   |   1    |                      [(0, 1)]                      |\n",
      "|   low-cost competition    |  1   |   2    |                  [(0, 1), (8, 1)]                  |\n",
      "|         wild ups          |  1   |   1    |                      [(0, 1)]                      |\n",
      "|          lacking          |  10  |   1    |                      [(1, 1)]                      |\n",
      "|     business advisers     |  1   |   1    |                      [(2, 1)]                      |\n",
      "|        two-pronged        |  1   |   1    |                      [(3, 1)]                      |\n",
      "|    regulatory assault     |  1   |   1    |                      [(3, 1)]                      |\n",
      "|   act forces companies    |  1   |   1    |                      [(4, 1)]                      |\n",
      "|      paperwork costs      |  1   |   1    |                      [(4, 1)]                      |\n",
      "|      exchange-listed      |  1   |   1    |                      [(5, 1)]                      |\n",
      "|           ifrs            |  1   |   1    |                      [(5, 1)]                      |\n",
      "|       across europe       |  1   |   1    |                      [(5, 1)]                      |\n",
      "|          hacking          |  5   |   1    |                      [(6, 1)]                      |\n",
      "|         red tape          |  4   |   1    |                      [(6, 1)]                      |\n",
      "|   low-cost competitors    |  1   |   1    |                      [(6, 1)]                      |\n",
      "|         low-wage          |  1   |   1    |                      [(7, 1)]                      |\n",
      "|    low-wage countries     |  1   |   1    |                      [(7, 1)]                      |\n",
      "|          % plan           |  1   |   1    |                      [(7, 1)]                      |\n",
      "|    significant threat     |  1   |   1    |                      [(8, 1)]                      |\n",
      "|         dipiazza          |  1   |   2    |                 [(9, 1), (12, 1)]                  |\n",
      "|         outsource         |  1   |   1    |                      [(9, 1)]                      |\n",
      "|  global chief executive   |  1   |   1    |                      [(9, 1)]                      |\n",
      "|      samuel dipiazza      |  1   |   1    |                      [(9, 1)]                      |\n",
      "|      outsource work       |  1   |   1    |                      [(9, 1)]                      |\n",
      "|       home markets        |  1   |   1    |                      [(9, 1)]                      |\n",
      "|           kill            |  15  |   1    |                     [(10, 1)]                      |\n",
      "|        frank brown        |  1   |   1    |                     [(10, 1)]                      |\n",
      "|  global advisory leader   |  1   |   1    |                     [(10, 1)]                      |\n",
      "|      large companies      |  6   |   1    |                     [(10, 1)]                      |\n",
      "|     global operations     |  1   |   1    |                     [(10, 1)]                      |\n",
      "|       clear upside        |  1   |   1    |                     [(10, 1)]                      |\n",
      "|      survey suggests      |  2   |   1    |                     [(11, 1)]                      |\n",
      "|       rapid decline       |  1   |   1    |                     [(11, 1)]                      |\n",
      "|        huge threat        |  2   |   1    |                     [(11, 1)]                      |\n",
      "|   third-largest problem   |  1   |   1    |                     [(11, 1)]                      |\n",
      "|          hedged           |  1   |   1    |                     [(12, 1)]                      |\n",
      "|        mr dipiazza        |  1   |   1    |                     [(12, 1)]                      |\n",
      "|        new reality        |  1   |   1    |                     [(12, 1)]                      |\n",
      "| favourable exchange rates |  1   |   1    |                     [(12, 1)]                      |\n",
      "|            324            |  2   |   1    |                     [(13, 1)]                      |\n",
      "+---------------------------+------+--------+----------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "document_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\", \"business\", \"509.txt\")\n",
    "\n",
    "print(I.doc_to_string(map_path_to_articleID(document_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization \n",
    "\n",
    "TF: \n",
    "* Document: Term frequencies are assessed on document level.\n",
    "* Sentence: Term frequencies are assessed on sentence level.\n",
    "\n",
    "IDF: Inverted document frequencies is assessed on collection level.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "TODO: \n",
    "* Evaluate choice and give reason: \n",
    "    * IDF on document level?\n",
    "    * TF on document level for sentences? \n",
    "* \"order\" parameter o\n",
    "* BM25\n",
    "* BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_term(N, df_t, tf_t_d):\n",
    "    return (1 + math.log10(tf_t_d)) * math.log10(N/df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_term(df_t, tf_t_d, N, s_len_avg, s_len, k, b): \n",
    "    idf_t = math.log10(N/df_t)\n",
    "    B = 1 - b + b * (s_len/s_len_avg)\n",
    "    return idf_t * (tf_t_d * (k + 1))/(tf_t_d + k * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(d: dict, max_sent: int, reverse=False) -> dict: \n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=reverse)[:max_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence: str, model, tokenizer, max_length=512) -> torch.tensor: \n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    output = model(**encoded_input)\n",
    "    embedding = output[\"pooler_output\"].squeeze()\n",
    "    # mean pooled embedding might be better\n",
    "    # mean_pooled_embedding = last_hidden_states.mean(axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summarization(d,p,l,o,I,args)\n",
    "    @input document d (the index in I/D), maximum number of sentences (p) and/or characters (l), order\n",
    "    of presentation o (appearance in text vs relevance), inverted index I or the\n",
    "    collection D, and optional arguments on IR models\n",
    "\n",
    "    @behavior preprocesses d, assesses the relevance of each sentence in d against I ac-\n",
    "    cording to args, and presents them in accordance with p, l and o\n",
    "    \n",
    "    @output summary s of document d, i.e. ordered pairs (sentence position in d, score)\n",
    "'''\n",
    "def summarization(d: int, p: int, l: int, o: int, I_or_D: Union[InvertedIndex, list], **args) -> list:\n",
    "\n",
    "    if args['model'] != 'BERT':\n",
    "\n",
    "        ## if we receive the collection instead of the inverted index we must compute it first\n",
    "        if type(I_or_D) == list:\n",
    "            I = indexing(I_or_D)         \n",
    "        else: \n",
    "            I = I_or_D\n",
    "        \n",
    "        doc_info = I.get_document_info(d)\n",
    "        sentence_lengths = I.get_sentence_lengths(d)\n",
    "        term_doc_info = zip(*doc_info.values())    \n",
    "\n",
    "    scores = defaultdict(int)\n",
    "    sq_normalization_term = defaultdict(int)\n",
    "\n",
    "    if args['model'] == 'TF-IDF':\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info:\n",
    "            rel_t_d = tf_idf_term(I.N, df_t, tf_t_d)\n",
    "            for sent_number, tf_s_t in tf_per_sentence:\n",
    "                scores[sent_number] += rel_t_d * (1 + math.log10(tf_s_t))\n",
    "                sq_normalization_term[sent_number] = (1 + math.log10(tf_s_t))**2\n",
    "        # normalization\n",
    "        for sent_number, score in scores.items():\n",
    "            scores[sent_number] = score / math.sqrt(sq_normalization_term[sent_number])\n",
    "    \n",
    "    elif args['model'] == 'BM25':\n",
    "        k = 0.2\n",
    "        b = 0.75 \n",
    "        sentence_lengths = I.sentence_term_counts[d]\n",
    "        avg_sentence_length = sum(sentence_lengths)/len(sentence_lengths)\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info: \n",
    "            for sent_number, tf_s_t in tf_per_sentence: \n",
    "                scores[sent_number] += BM25_term(df_t, tf_s_t, I.N, avg_sentence_length, sentence_lengths[sent_number], k, b)\n",
    "    \n",
    "    elif args['model'] == 'BERT':\n",
    "        document = I_or_D[d]\n",
    "        tokenizer = args['bert_tokenizer']\n",
    "        bert_model = args['bert_model']\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        bert_model.to(device)\n",
    "        scores = defaultdict(float)\n",
    "        # sentences \n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        sentences = sentences[1:]\n",
    "        num_sentences = len(sentences)\n",
    "        sent_embeddings = list()\n",
    "        # every sentences on its own, no padding needed, faster on cpu\n",
    "        # for gpu batches are better \n",
    "        for sent in sentences: \n",
    "            sent_embedding = get_embedding(sent, bert_model, tokenizer)\n",
    "            sent_embeddings.append(sent_embedding)\n",
    "        # document\n",
    "        doc_embedding = get_embedding(document, bert_model, tokenizer, max_length=512)\n",
    "        for sent_id in range(0, num_sentences): \n",
    "            sent_vec = sent_embeddings[sent_id]\n",
    "            score = torch.nn.functional.cosine_similarity(doc_embedding, sent_vec, dim=0)\n",
    "            scores[sent_id] = score.item()\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Currently we only support the following models:\\n→ TF-IDF\\n→ BM-25\\n→ BERT\")\n",
    "    \n",
    "    sorted_scores = sort_by_value(scores, max_sent=p, reverse=True)\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=I, model='TF-IDF')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "sentences = sentences[1:]\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=I, model='BM25')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "sentences = sentences[1:]\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=articles, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "sentences = sentences[1:]\n",
    "for sent_id, score in scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "Calculates the keywords based on the tf-idf of the document.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "Parameter for including only noun phrases. \n",
    "\n",
    "No need of BERT (see assigment sheet, p.4 IR Models)\n",
    "\n",
    "should be primarly based on TF-IDF\n",
    "\n",
    "\n",
    "TODO:\n",
    "* Nouns: just unigrams or also bigrams?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import Senna\n",
    "from nltk.tag import SennaChunkTagger\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "keyword extraction(d,p,I,args)\n",
    "    @input document d, maximum number of keywords p, inverted index I, and op-\n",
    "    tional arguments on IR model choices\n",
    "\n",
    "    @behavior extracts the top informative p keywords in d against I according to args\n",
    "    \n",
    "    @output ordered set of p keywords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(d: int, p: int ,I: InvertedIndex, **args) -> dict:\n",
    "     \n",
    "    doc_info = I.get_document_info(d)\n",
    "    term_doc_info = zip(*doc_info.values())    \n",
    "\n",
    "    scores = defaultdict(str)\n",
    "\n",
    "    for term, df_t, tf_t_d, tf_per_sentence in term_doc_info:\n",
    "        rel_t_d = tf_idf_term(I.N, df_t, tf_t_d)\n",
    "        scores[term] = rel_t_d\n",
    "    scores = sort_by_value(scores, p, reverse=True)\n",
    "    return scores         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "scores = keyword_extraction(article_id, 10, I)\n",
    "print(scores)\n",
    "\n",
    "print(I.doc_to_string(article_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "TODO:\n",
    "* Implement evaluation\n",
    "* Evaluation:\n",
    "    * Statistics \n",
    "    * F-meassure\n",
    "    * Recall-precision-curve\n",
    "    * MAP\n",
    "    * Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "evaluation(Sset,Rset,args)\n",
    "    @input the set of summaries Sset produced from selected documents Dset ⊆ D\n",
    "    (e.g. a single document, a category of documents, the whole collection),\n",
    "    the corresponding reference extracts Rset, and optional arguments (evalu-\n",
    "    ation, preprocessing, model options)\n",
    "\n",
    "    @behavior assesses the produced summaries against the reference ones using the tar-\n",
    "    get evaluation criteria\n",
    "\n",
    "    @output evaluation statistics, including F-measuring at predefined p-or-l summary\n",
    "    limits, recall-and-precision curves, MAP, and efficiency\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(S: list, R: list, args=None) -> list:\n",
    "    # do evaluation... \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset for BERT comparison\\\n",
    "takes too long otherwise\\ \n",
    "\n",
    "TODO: \n",
    "* evaluation + first question -> Sebastian   \n",
    "* question 2, 3, 4 -> Francisco\n",
    "* question 5 and 6 -> Tuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "summarization(articles[article_id], num_sent=5, order=\"rel\", inverted_index=inverted_index, N=len(articles), article_id=article_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "keyword_extraction(articles[0], 10, inverted_index, len(articles), 0, use_only_nouns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
