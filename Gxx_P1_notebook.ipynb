{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 11**\n",
    "- Francisco Martins, 99068\n",
    "- Tunahan Güneş, 108108\n",
    "- Sebastian Weidinger, 111612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(text: str) -> str: \n",
    "    text = text.split(\"\\n\\n\")\n",
    "    # remove title\n",
    "    text = text[1:]\n",
    "    text = \" \".join(text)\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    text = text.replace(\" !\", \"!\")\n",
    "    text = re.sub(r'\\s+', r' ', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary(text: str) -> str: \n",
    "    text = text.replace(\" ?\", \"?\")\n",
    "    text = text.replace(\" !\", \"!\")\n",
    "    text = re.sub(r'\\s+', r' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(article_path, summary_path):\n",
    "    articles = []\n",
    "    summaries = []\n",
    "    article_file_paths = []\n",
    "    summary_file_paths = []\n",
    "    category_names = list()\n",
    "    for folder in os.listdir(article_path):\n",
    "        category_names.append(folder)\n",
    "        article_category_path = os.path.join(article_path, folder)\n",
    "        summary_category_path = os.path.join(summary_path, folder)\n",
    "        articles.append([])\n",
    "        summaries.append([])\n",
    "        article_file_paths.append([])\n",
    "        summary_file_paths.append([])\n",
    "        for file in os.listdir(article_category_path):\n",
    "            article_file_path = os.path.join(article_category_path, file)\n",
    "            summary_file_path = os.path.join(summary_category_path, file)\n",
    "            article_file_paths[-1].append(article_file_path)\n",
    "            summary_file_paths[-1].append(summary_file_path)\n",
    "            # articles\n",
    "            with open(article_file_path, \"r\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                text = preprocess_article(text)\n",
    "                articles[-1].append(text)\n",
    "            #summaries \n",
    "            with open(summary_file_path, \"r\", errors=\"ignore\") as f: \n",
    "                text = f.read()\n",
    "                text = preprocess_summary(text)\n",
    "                summaries[-1].append(text)\n",
    "                \n",
    "    print(\"Number of Categories:\",len(os.listdir(article_path)))\n",
    "    for i in range(len(os.listdir(article_path))):\n",
    "        print(\"Number of Articles in\", \"'\"+os.listdir(article_path)[i]+\"'\", \"Category:\",len(articles[i]))\n",
    "    \n",
    "    return article_file_paths, articles, summary_file_paths, summaries, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\")\n",
    "summary_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"Summaries\")\n",
    "print(\"Article path:\", article_path)\n",
    "print(\"Summary path:\", summary_path)\n",
    "article_file_paths, categorized_articles, summary_file_paths, categorized_summaries, category_names= read_files(article_path, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examplary text. The structure of the read file is: articles[category_no][document_no]. \n",
    "print(categorized_articles[0][0])\n",
    "print(article_file_paths[508:512])\n",
    "print(categorized_summaries[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lists) -> list: \n",
    "    return [element for sublist in lists for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_sentence_indices(articles: list, summaries: list) -> list: \n",
    "    categorized_summary_indices = list()\n",
    "    categorized_article_summary = list(zip(articles, summaries))\n",
    "    found_summary = 0\n",
    "    faulty_summaries = list()\n",
    "    for category_id, category in enumerate(categorized_article_summary): \n",
    "        article_summary_tuples = list(zip(category[0], category[1]))\n",
    "        categorized_summary_indices.append([])\n",
    "        for article_id, (article, summary) in enumerate(article_summary_tuples):\n",
    "            sentence_indices = list()\n",
    "            recreated_summary = \"\"\n",
    "            article_sents = sent_tokenize(article)\n",
    "            article_sents = set(article_sents)\n",
    "            for sent_id, sent in enumerate(article_sents): \n",
    "                if summary.find(sent) != -1: \n",
    "                    sentence_indices.append(sent_id)\n",
    "                    recreated_summary += sent\n",
    "            categorized_summary_indices[-1].append(sentence_indices)\n",
    "            summary_length = len(summary)\n",
    "            recreated_summary_length = len(recreated_summary)\n",
    "            if abs(summary_length - recreated_summary_length) < 3: \n",
    "                found_summary += 1\n",
    "            else: \n",
    "                faulty_summaries.append((category_id, article_id))\n",
    "    print(f\"number of found summaries: {found_summary}\")\n",
    "    print(f\"number of summaries: {len(flatten(summaries))}\")\n",
    "    print(f\"{float(found_summary)/float(len(flatten(summaries))) * 100 :.2f}%\")\n",
    "    return categorized_summary_indices, faulty_summaries\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorized_summary_sentence_indices, faulty_documents = get_summary_sentence_indices(categorized_articles, categorized_summaries)\n",
    "categorized_summary_sentence_indices, faulty_summary_ids = get_summary_sentence_indices(categorized_articles, categorized_summaries)\n",
    "print(categorized_summary_sentence_indices)\n",
    "print(faulty_summary_ids)\n",
    "#print(len(faulty_documents)/len(flatten(categorized_summary_sentence_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entries(categorized_list: list, faulty_summary_ids: list): \n",
    "    for category_id, sent_id in faulty_summary_ids:\n",
    "        del categorized_list[category_id][sent_id] \n",
    "    return categorized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(categorized_articles[1]))\n",
    "categorized_articles = remove_entries(categorized_articles, faulty_summary_ids)\n",
    "article_file_paths = remove_entries(article_file_paths, faulty_summary_ids)\n",
    "print(len(categorized_articles[1]))\n",
    "print(len(categorized_summaries[1]))\n",
    "categorized_summaries = remove_entries(categorized_summaries, faulty_summary_ids)\n",
    "summary_file_paths = remove_entries(summary_file_paths, faulty_summary_ids)\n",
    "print(len(categorized_summaries[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_sentence_indices = flatten(categorized_summary_sentence_indices)\n",
    "with open(\"./testing/reference/categorized_summary_sentence_indices.json\", \"w\") as f: \n",
    "    json.dump(categorized_summary_sentence_indices, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from typing import Union\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter, defaultdict\n",
    "from tabulate import tabulate\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list to get uncategorized collection\n",
    "articles = flatten(categorized_articles)\n",
    "summaries = flatten(categorized_summaries)\n",
    "N = len(articles)\n",
    "N_summaries = len(summaries)\n",
    "#article_file_paths = flatten(article_file_paths)\n",
    "dict_path_to_articleID = {path:i for i, path in enumerate(flatten(article_file_paths))}\n",
    "\n",
    "def map_path_to_articleID(path):\n",
    "    path = os.path.normpath(path)\n",
    "    return dict_path_to_articleID.get(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index Structure \n",
    " \n",
    "Each term points to a dictionary of document identifier and the term frequency in the document.\n",
    "\n",
    "t1 -> {doc1: TF, doc5: TF, ...}\\\n",
    "t2 -> {doc7: TF, doc8: TF, ...}\\\n",
    "...\n",
    "t2 -> [DF, {doc7: [TF_(t2, doc7), {s1: TF, s4: TF, ...}], doc8: [TF_(t2, doc8), {s2: TF, s4: TF, ...}], ...}]\\\n",
    "\n",
    "use class structure\n",
    "\n",
    "TODO: \n",
    "* Optimize structure?\n",
    "    * Is there a more efficient way? \n",
    "    * Add maybe pointers to sentences and their term frequency? -> Faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermFrequencies: \n",
    "    def __init__(self) -> None:\n",
    "        self.tf_d_t = 0\n",
    "        self.sent_tf = list()\n",
    "\n",
    "    def add_sentence(self, sent_number, term_frequency):\n",
    "        self.sent_tf.append((sent_number, term_frequency))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        padding = 5 - len(str(self.tf_d_t))\n",
    "        return f'TF_d_t: {self.tf_d_t}{\" \" * padding}TF_per_sentence: {self.sent_tf}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexEntry:\n",
    "    def __init__(self) -> None:\n",
    "        self.df_term = 0\n",
    "        self.term_dict = defaultdict(TermFrequencies)\n",
    "    \n",
    "    def get_document(self, document):\n",
    "        return self.term_dict.get(document, None)\n",
    "\n",
    "    def get_or_default_document(self, document):\n",
    "        return self.term_dict[document]\n",
    "\n",
    "    def update_document(self, document, new_value):\n",
    "        self.term_dict[document] = new_value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Document Frequency: {self.df_term}\\n {\" \" * (max_width+2)} Term frequencies:\\n'\n",
    "        for doc_number, tfs in self.term_dict.items():\n",
    "            padding = 5 - len(str(doc_number))\n",
    "            out += f'{\" \" * (max_width + 3)} Doc {doc_number}{\" \" * padding}→ {tfs}\\n'\n",
    "        return out\n",
    "    \n",
    "    def calculate_df(self):\n",
    "        self.df_term = len(self.term_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, collection_size, doc_lengths) -> None:\n",
    "        self.inverted_index = defaultdict(InvertedIndexEntry)\n",
    "        self.sentence_term_counts = list()\n",
    "        self.sentence_num_chars = list()\n",
    "        self.indexing_time = 0\n",
    "        self.N = collection_size\n",
    "        self.doc_lengths = np.array(doc_lengths)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f'Time to index: {self.indexing_time}\\nInverted Index:\\n'\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            padding = max_width - len(term)\n",
    "            out += f'{term} {\" \" * padding} → {entry}\\n'\n",
    "        return out\n",
    "    \n",
    "    def get_or_default(self, term, document):\n",
    "        return self.inverted_index[term].get_or_default_document(document)\n",
    "    \n",
    "    def update(self, term, document, new_value):\n",
    "        self.inverted_index[term].update_document(document, new_value)\n",
    "    \n",
    "    def set_indexing_time(self, indexing_time):\n",
    "        self.indexing_time = indexing_time\n",
    "    \n",
    "    def calculate_dfs(self):\n",
    "        for entry in self.inverted_index.values():\n",
    "            entry.calculate_df()  \n",
    "    \n",
    "    def get_sentence_lengths(self, document):\n",
    "        return self.sentence_term_counts[document]\n",
    "\n",
    "    def get_document_info(self, document):          \n",
    "        info = {'Vocabulary': [], 'DF_t': [], 'TF_d_t': [], 'TF/sentence': []}\n",
    "        for term, entry in self.inverted_index.items():\n",
    "            doc_tfs = entry.get_document(document)\n",
    "            if doc_tfs == None:\n",
    "                continue\n",
    "            info['Vocabulary'].append(term)\n",
    "            info['DF_t'].append(entry.df_term)\n",
    "            info['TF_d_t'].append(doc_tfs.tf_d_t)\n",
    "            info['TF/sentence'].append(doc_tfs.sent_tf)\n",
    "        return info\n",
    "    \n",
    "    def doc_to_string(self, document: int):\n",
    "        out = f'Document id={document} → vocabulary and term frequencies:\\n'\n",
    "        info = self.get_document_info(document)\n",
    "        table = zip(*info.values())\n",
    "        headers = info.keys()\n",
    "        return out + tabulate(table, headers, tablefmt=\"pretty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(sentence):\n",
    "    sents = list()\n",
    "    for paragraph in sentence.split('\\n '):\n",
    "        # split into sentences  \n",
    "        sents_p = nltk.sent_tokenize(paragraph)\n",
    "        for sent in sents_p:\n",
    "            sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@ input a sentence to process, a tokenizer to split it into terms, a lemmatizer to normalize the terms,\n",
    "a set consisting of stop_words to ignore\n",
    "\n",
    "@behavior preprocesses the sentence\n",
    "\n",
    "@output a triple consisting of the length in characters of the sentence, the number of terms in the sentence and\n",
    "a list of terms and noun phrases appearing in the sentence (with repeated terms) \n",
    "'''\n",
    "def preprocess(sentence: str, tokenizer: nltk.tokenize.api.TokenizerI, wnl: WordNetLemmatizer, stop_words=set):\n",
    "    sent_out = list()\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
    "    for term in tokenized_sentence:\n",
    "        lem_term = wnl.lemmatize(term)\n",
    "        if lem_term not in stop_words:      \n",
    "            sent_out.append(lem_term)\n",
    "    blobbed_sentence = TextBlob(sentence)\n",
    "    all_noun_phrases = blobbed_sentence.noun_phrases\n",
    "    # only include those that have multiple words\n",
    "    noun_phrases = [n_p for n_p in all_noun_phrases if ' ' in n_p]\n",
    "    sent_out.extend(noun_phrases)\n",
    "    return len(sentence), len(tokenized_sentence), sent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "indexing(D,args)\n",
    "    @input document collection D and optional arguments on text preprocessing\n",
    "\n",
    "    @behavior preprocesses the collection and, using existing libraries, \n",
    "    builds an inverted index with the relevant statistics for the subsequent summarization functions\n",
    "    \n",
    "    @output pair with the inverted index I and indexing time\n",
    "'''\n",
    "def indexing(articles, **args) -> InvertedIndex:\n",
    "    start_time = time.time()\n",
    "    inverted_index = InvertedIndex(len(articles), [len(article) for article in articles])\n",
    "\n",
    "    # tokenizer split words and keep hyphens e.g. state-of-the-art\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|-]+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # loop through collection \n",
    "    for article_id, article in enumerate(articles): \n",
    "        sents = sentence_tokenize(article)\n",
    "        # remove title (not needed for summarization task)\n",
    "        #sents = sents[1:]\n",
    "        preprocessing_results = [preprocess(sent, tokenizer, wnl, stop_words) for sent in sents]\n",
    "        sent_num_chars, sent_term_counts, preprocessed_sentences = zip(*preprocessing_results)\n",
    "        inverted_index.sentence_num_chars.append(list(sent_num_chars))\n",
    "        inverted_index.sentence_term_counts.append(list(sent_term_counts))\n",
    "        # count the term frequencies per sentence\n",
    "        term_counter_per_sent = [Counter(sentence_terms) for sentence_terms in preprocessed_sentences]\n",
    "        for sent_number, term_counter in enumerate(term_counter_per_sent):\n",
    "            for term in term_counter: \n",
    "                tf = term_counter[term]\n",
    "                term_document_tfs = inverted_index.get_or_default(term, article_id)\n",
    "                term_document_tfs.tf_d_t += tf \n",
    "                term_document_tfs.add_sentence(sent_number, tf)\n",
    "                inverted_index.update(term, article_id, term_document_tfs)\n",
    "    inverted_index.calculate_dfs()\n",
    "    end_time = time.time()\n",
    "    indexing_time = end_time - start_time\n",
    "    inverted_index.set_indexing_time(indexing_time)\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize(\"played\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_removed = 'Hello, I\\'m a data scientist and machine learning enthusiast. My name is John Williams and am a data scientist. The majestic, centuries-old oak tree stood proudly at the edge of the meadow'\n",
    "blob = TextBlob(sent_removed)\n",
    "[a for a in blob.noun_phrases if ' ' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'Title. The little white little rabbit. The person played with the ball.'\n",
    "s1 = 'Title. The white rabbit\\'s ball. Rabbit rabbit ball rabbit.'\n",
    "s2 = 'Title.  White, the little white rabbit. Little, little.'\n",
    "test = [s0, s1, s2]\n",
    "I_test = indexing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I_test.doc_to_string(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = indexing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I.sentence_term_counts[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = os.path.join(\"BBC News Summary\", \"BBC News Summary\", \"News Articles\", \"business\", \"509.txt\")\n",
    "\n",
    "print(I.doc_to_string(map_path_to_articleID(document_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization \n",
    "\n",
    "TF: \n",
    "* Document: Term frequencies are assessed on document level.\n",
    "* Sentence: Term frequencies are assessed on sentence level.\n",
    "\n",
    "IDF: Inverted document frequencies is assessed on collection level.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "TODO: \n",
    "* Evaluate choice and give reason: \n",
    "    * IDF on document level?\n",
    "    * TF on document level for sentences? \n",
    "* \"order\" parameter o\n",
    "* BM25\n",
    "* BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10_tf(x):\n",
    "    try:\n",
    "        return (1 + np.log10(x)) \n",
    "    except ValueError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_term(N, df_t, tf_t_d):\n",
    "    return log10_tf(tf_t_d) * np.log10(N/df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_relevance_scores(info: list, N: int):\n",
    "    scores = defaultdict(int)\n",
    "    sq_normalization_term = defaultdict(int)\n",
    "    for _, df_t, tf_t_d, tf_per_sentence in info:\n",
    "        rel_t_d = tf_idf_term(N, df_t, tf_t_d)\n",
    "        for sent_number, tf_s_t in tf_per_sentence:\n",
    "            scores[sent_number] += rel_t_d * log10_tf(tf_s_t)\n",
    "            sq_normalization_term[sent_number] = log10_tf(tf_s_t)**2\n",
    "    # normalization\n",
    "    for sent_number, score in scores.items():\n",
    "        scores[sent_number] = score / np.sqrt(sq_normalization_term[sent_number])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities_between_sentences(info: list, N: int, num_sentences: int):\n",
    "    similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "    sq_norm_sentence = np.zeros(num_sentences)\n",
    "    for _, df_t, tf_t_d, tf_per_sentence in info:\n",
    "        log10_tfs = np.zeros(num_sentences)\n",
    "        for sent_number, tf_s_t in tf_per_sentence:\n",
    "            log10_tfs[sent_number] = log10_tf(tf_s_t)\n",
    "        scores_for_term = np.log10(N/df_t) * np.outer(log10_tfs, log10_tfs)\n",
    "        similarity_matrix += scores_for_term\n",
    "        sq_norm_sentence += log10_tfs**2\n",
    "    norm_sentence = np.sqrt(sq_norm_sentence)\n",
    "    normalization = np.outer(norm_sentence, norm_sentence)\n",
    "    return similarity_matrix / normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_term(df_t, tf_t_d, N, s_len_avg, s_len, k, b): \n",
    "    idf_t = np.log10(N/df_t)\n",
    "    B = 1 - b + b * (s_len/s_len_avg)\n",
    "    return idf_t * (tf_t_d * (k + 1))/(tf_t_d + k * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(d: dict, max_elements: int, reverse=False) -> dict: \n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=reverse)[:max_elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_key(d: dict) -> dict: \n",
    "    return dict(sorted(d.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_sort(scores: dict, o: str, p: int, l: int, sentence_lengths: list): \n",
    "    # Don't exceed maximum number of sentences. If it doesn't matter it should be set to 0\n",
    "    max_elements = p if p else len(scores)\n",
    "    sorted_scores = sort_by_value(scores, max_elements=max_elements, reverse=True)\n",
    "\n",
    "    # Don't exceed maximum number of characters. If it doesn't matter it should be set to 0\n",
    "    if l:\n",
    "        total_length = 0\n",
    "        cropped_sorted_scores = dict()\n",
    "        for sent_number, score in sorted_scores.items():\n",
    "            sent_length = sentence_lengths[sent_number]\n",
    "            total_length += sent_length\n",
    "            if total_length > l:\n",
    "                break\n",
    "            cropped_sorted_scores[sent_number] = score\n",
    "        sorted_scores = cropped_sorted_scores\n",
    "\n",
    "    if o == \"rel\": \n",
    "        return sorted_scores\n",
    "    elif o == \"app\": \n",
    "        return sort_by_key(sorted_scores)\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence: str, model, tokenizer, device, max_length=512) -> torch.tensor: \n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    encoded_input.to(device)\n",
    "    output = model(**encoded_input)\n",
    "    embedding = output[\"pooler_output\"].squeeze()\n",
    "    # mean pooled embedding might be better\n",
    "    # mean_pooled_embedding = last_hidden_states.mean(axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentences: list, model, tokenizer, device, max_length=512) -> list: \n",
    "    encoded_input = tokenizer(sentences, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    encoded_input.to(device)\n",
    "    output = model(**encoded_input)\n",
    "    embedding = output[\"pooler_output\"].squeeze()\n",
    "    # mean pooled embedding might be better\n",
    "    # mean_pooled_embedding = last_hidden_states.mean(axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_info_after_sent_removal(info: list, sent_removed: int):\n",
    "    for i, (term, df_t, tf_t_d, tf_per_sentence) in enumerate(info):\n",
    "        # decrease term frequency of info of the terms in the removed sentence\n",
    "        info[i][2] -= tf_per_sentence[sent_removed]  \n",
    "        # remove the sentence from info\n",
    "        info[i][3] = [(sent_number, score) for sent_number, score in info[i][3] if sent_number != sent_removed]\n",
    "        if info[i][2] == 0:\n",
    "            info[i][1] -= 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "summarization(d,p,l,o,I,args)\n",
    "    @input document d (the index in I/D), maximum number of sentences (p) and/or characters (l), order\n",
    "    of presentation o (appearance in text vs relevance), inverted index I or the\n",
    "    collection D, and optional arguments on IR models\n",
    "\n",
    "    @behavior preprocesses d, assesses the relevance of each sentence in d against I ac-\n",
    "    cording to args, and presents them in accordance with p, l and o\n",
    "    \n",
    "    @output summary s of document d, i.e. ordered pairs (sentence position in d, score)\n",
    "'''\n",
    "def summarization(d: int, p: int, l: int, o: int, I_or_D: Union[InvertedIndex, list], **args) -> list:\n",
    "\n",
    "    ## if we receive the collection instead of the inverted index we must compute it first\n",
    "    if type(I_or_D) == list:\n",
    "        I = indexing(I_or_D)         \n",
    "    else: \n",
    "        I = I_or_D\n",
    "    model =  ('model' in args and args['model']) or 'TF-IDF'\n",
    "        \n",
    "    doc_info = I.get_document_info(d)\n",
    "    term_doc_info = zip(*doc_info.values())      \n",
    "    sentence_lengths = I.get_sentence_lengths(d)\n",
    "    num_sentences = len(sentence_lengths)\n",
    "    scores = defaultdict(int)\n",
    "\n",
    "    if model == 'TF-IDF':\n",
    "        scores = tf_idf_relevance_scores(term_doc_info, I.N)\n",
    "    \n",
    "    elif model == 'BM25':\n",
    "        k =  ('k' in args and args['k']) or 0.2\n",
    "        b = ('b' in args and args['b']) or 0.75\n",
    "        avg_sentence_length = sum(sentence_lengths)/len(sentence_lengths)\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info: \n",
    "            for sent_number, tf_s_t in tf_per_sentence: \n",
    "                scores[sent_number] += BM25_term(df_t, tf_s_t, I.N, avg_sentence_length, sentence_lengths[sent_number], k, b)\n",
    "    \n",
    "    elif model == 'BERT':\n",
    "        document = I_or_D[d]\n",
    "        \n",
    "        tokenizer = args['bert_tokenizer']\n",
    "        bert_model = args['bert_model']\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        bert_model.to(device)\n",
    "        \n",
    "        scores = defaultdict(float)\n",
    "        # sentences \n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        #sentences = sentences[1:]\n",
    "        sent_embeddings = list()\n",
    "        # every sentences on its own, no padding needed, faster on cpu\n",
    "        # for gpu batches are better \n",
    "        sent_embeddings = get_embeddings(sentences, bert_model, tokenizer, device)\n",
    "        #for sent in sentences: \n",
    "        #    sent_embedding = get_embedding(sent, bert_model, tokenizer, device)\n",
    "        #    sent_embeddings.append(sent_embedding)\n",
    "        # document\n",
    "        doc_embedding = get_embedding(document, bert_model, tokenizer, device, max_length=512)\n",
    "        for sent_id in range(0, num_sentences): \n",
    "            sent_vec = sent_embeddings[sent_id]\n",
    "            score = torch.nn.functional.cosine_similarity(doc_embedding, sent_vec, dim=0)\n",
    "            scores[sent_id] = score.item()\n",
    "    \n",
    "    elif model == 'MMR-TFIDF':\n",
    "        λ =  ('λ' in args and args['λ']) or 0.5\n",
    "        # if p is 0 it means select all sentences\n",
    "        runs = min(p, num_sentences) or num_sentences\n",
    "        term_doc_info = list(zip(*doc_info.values()))\n",
    "        similarity_matrix = compute_similarities_between_sentences(term_doc_info, I.N, num_sentences)\n",
    "        available_sentence_relevance = tf_idf_relevance_scores(term_doc_info, I.N)\n",
    "        selected_sentences = defaultdict(int)\n",
    "        selected_sentences_bool = np.zeros(num_sentences)\n",
    "        for _ in range(runs):\n",
    "            max_score = np.NINF\n",
    "            best_sentence = None\n",
    "            for sent_num, rel_score in available_sentence_relevance.items():\n",
    "                redundancy = np.max(similarity_matrix[sent_num, :], where=selected_sentences_bool == 1, initial=0)\n",
    "                mmr_score = (1-λ) * rel_score - λ * redundancy\n",
    "                if mmr_score > max_score:\n",
    "                    max_score = mmr_score\n",
    "                    best_sentence = sent_num\n",
    "            selected_sentences[best_sentence] = max_score\n",
    "            available_sentence_relevance.pop(best_sentence)\n",
    "            selected_sentences_bool[best_sentence] = 1\n",
    "        scores = selected_sentences\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Currently we only support the following models for summarization:\\n→ TF-IDF\\n→ BM-25\\n→ BERT\\n→MMR-TFIDF\")\n",
    "    \n",
    "    return select_and_sort(scores, o, p, l, sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6]])\n",
    "np.max(a[1:], where=np.array([1,1,0]) == 1, initial=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "all_scores = summarization(d=article_id, p=7, l=1000, o=\"app\", I_or_D=I, model='TF-IDF')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in all_scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "all_scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=I, model='BM25')\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in all_scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "all_scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=articles, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, score in all_scores.items(): \n",
    "    print(f\"{score:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "all_scores = list()\n",
    "for category_id, category in enumerate(article_file_paths): \n",
    "    all_scores.append([])\n",
    "    for path in category: \n",
    "        article_id = map_path_to_articleID(path)\n",
    "        article_score = summarization(d=article_id, p=7, l=1000, o=\"rel\", I_or_D=I, model='TF-IDF')\n",
    "        all_scores[-1].append(article_score)\n",
    "with open('./testing/extracts/tf_idf/categorized_scores.json', 'w') as f: \n",
    "    json.dump(all_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "all_scores = list()\n",
    "for category_id, category in enumerate(article_file_paths): \n",
    "    all_scores.append([])\n",
    "    for path in category: \n",
    "        article_id = map_path_to_articleID(path)\n",
    "        article_score = summarization(d=article_id, p=7, l=1000, o=\"rel\", I_or_D=I, model='BM25')\n",
    "        all_scores[-1].append(article_score)\n",
    "with open('./testing/extracts/bm25/categorized_scores.json', 'w') as f: \n",
    "    json.dump(all_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "all_scores = list()\n",
    "for category_id, category in enumerate(article_file_paths): \n",
    "    all_scores.append([])\n",
    "    for path in category: \n",
    "        article_id = map_path_to_articleID(path)\n",
    "        article_scores = summarization(d=article_id, p=5, l=1000, o=\"rel\", I_or_D=articles, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "        all_scores[-1].append(article_score)\n",
    "with open('./testing/extracts/bert/categorized_scores.json', 'w') as f: \n",
    "    json.dump(all_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "Calculates the keywords based on the tf-idf of the document.\\\n",
    "\\\n",
    "Additional parameter \"N\" and \"article_id\". Is this allowed?\n",
    "\n",
    "Parameter for including only noun phrases. \n",
    "\n",
    "No need of BERT (see assigment sheet, p.4 IR Models)\n",
    "\n",
    "should be primarly based on TF-IDF\n",
    "\n",
    "\n",
    "TODO:\n",
    "* Nouns: just unigrams or also bigrams?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "keyword extraction(d,p,I,args)\n",
    "    @input document d, maximum number of keywords p, inverted index I, and op-\n",
    "    tional arguments on IR model choices\n",
    "\n",
    "    @behavior extracts the top informative p keywords in d against I according to args\n",
    "    \n",
    "    @output ordered set of p keywords\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(d: int, p: int, I: InvertedIndex, **args) -> dict:\n",
    "     \n",
    "    model =  ('model' in args and args['model']) or 'TF-IDF'\n",
    "    doc_info = I.get_document_info(d)\n",
    "    term_doc_info = zip(*doc_info.values())    \n",
    "\n",
    "    scores = defaultdict(str)\n",
    "\n",
    "    if model == 'TF-IDF':\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info:\n",
    "            rel_t_d = tf_idf_term(I.N, df_t, tf_t_d)\n",
    "            scores[term] = rel_t_d\n",
    "\n",
    "    elif model == 'BM25':\n",
    "        k =  ('k' in args and args['k']) or 0.2\n",
    "        b = ('b' in args and args['b']) or 0.75\n",
    "        avg_doc_size = np.sum(I.doc_lengths)/I.N\n",
    "        for term, df_t, tf_t_d, tf_per_sentence in term_doc_info:\n",
    "            rel_t_d += BM25_term(df_t, tf_t_d, I.N, avg_doc_size, I.doc_lengths[d], k, b)\n",
    "            scores[term] = rel_t_d\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Currently we only support the following models for keyword extraction:\\n→ TF-IDF\\n→ BM-25\")\n",
    "\n",
    "    scores = sort_by_value(scores, p, reverse=True)\n",
    "    return scores         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = map_path_to_articleID(document_path)\n",
    "all_scores = keyword_extraction(article_id, 10, I, model='TF-IDF')\n",
    "print(all_scores)\n",
    "\n",
    "print(I.doc_to_string(article_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "TODO:\n",
    "* Implement evaluation\n",
    "* Evaluation:\n",
    "    * Statistics \n",
    "    * F-meassure\n",
    "    * Recall-precision-curve\n",
    "    * MAP\n",
    "    * Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "evaluation(Sset,Rset,args)\n",
    "    @input the set of summaries Sset produced from selected documents Dset ⊆ D\n",
    "    (e.g. a single document, a category of documents, the whole collection),\n",
    "    the corresponding reference extracts Rset, and optional arguments (evalu-\n",
    "    ation, preprocessing, model options)\n",
    "\n",
    "    @behavior assesses the produced summaries against the reference ones using the tar-\n",
    "    get evaluation criteria\n",
    "\n",
    "    @output evaluation statistics, including F-measuring at predefined p-or-l summary\n",
    "    limits, recall-and-precision curves, MAP, and efficiency\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    return len(relevant_indices.intersection(extracted_indices))/len(extracted_indices)\n",
    "\n",
    "def get_recall(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    return len(relevant_indices.intersection(extracted_indices))/len(relevant_indices)\n",
    "\n",
    "def get_F1(extracted_indices, relevant_indices):\n",
    "    extracted_indices = set(extracted_indices)\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    p = get_precision(extracted_indices, relevant_indices)\n",
    "    r = get_recall(extracted_indices, relevant_indices)\n",
    "    if p+r == 0: return 0\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_per_category(f1_per_category, std_per_category, category_names, title):\n",
    "    x = category_names\n",
    "    color = [\"red\", \"blue\", \"green\", \"orange\", \"magenta\"]\n",
    "    plt.bar(x, f1_per_category, color=color)\n",
    "    plt.errorbar(x, f1_per_category, std_per_category, fmt='.', color='Black', elinewidth=2,capthick=10,errorevery=1, alpha=0.5, ms=4, capsize = 2)\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_curve_data(extracted_indices, relevant_indices):\n",
    "    extracted_indices = list(extracted_indices.keys())\n",
    "    extracted_indices = list(map(int, extracted_indices))\n",
    "    relevant_indices = set(relevant_indices)\n",
    "    precision_level = list()\n",
    "    recall_level = list()\n",
    "    for k in range(len(extracted_indices)): \n",
    "        extracted_indices_at_k = set(extracted_indices[:k+1])\n",
    "        intersect = extracted_indices_at_k.intersection(relevant_indices)\n",
    "        precision = len(intersect)/len(extracted_indices_at_k)\n",
    "        precision_level.append(precision)\n",
    "        recall_level.append((k+1)/len(extracted_indices))\n",
    "    return precision_level, recall_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precision_level, recall_level): \n",
    "    plt.plot(recall_level, precision_level)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1.01)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(S: list, R: list, **args) -> list:\n",
    "    precisions = list()\n",
    "    recalls = list() \n",
    "    f1_scores = list()\n",
    "    for category_id, (category_extracted_indices_rel, category_relevant_indices) in enumerate(zip(R, S)):\n",
    "            precisions.append([])\n",
    "            recalls.append([])\n",
    "            f1_scores.append([])\n",
    "            for extracted_indices_rel, relevant_indices in zip(category_extracted_indices_rel, category_relevant_indices):\n",
    "                extracted_indices = list(extracted_indices_rel.keys())\n",
    "                extracted_indices = list(map(int, extracted_indices))\n",
    "                precisions[-1].append(get_precision(extracted_indices, relevant_indices))\n",
    "                recalls[-1].append(get_recall(extracted_indices, relevant_indices))\n",
    "                f1_scores[-1].append(get_F1(extracted_indices, relevant_indices))\n",
    "    \n",
    "    mean_f1_per_catgory = list()\n",
    "    mean_std_per_category = list()\n",
    "    for f1 in f1_scores: \n",
    "        mean_f1_per_catgory.append(statistics.mean(f1))\n",
    "        mean_std_per_category.append(statistics.stdev(f1))\n",
    "\n",
    "    plot_f1_per_category(mean_f1_per_catgory, mean_std_per_category, args['category_names'], args['model_name'])\n",
    "\n",
    "    mean_precision = statistics.mean(flatten(precisions))\n",
    "    mean_recall = statistics.mean(flatten(recalls))\n",
    "    mean_f1_scores = statistics.mean(flatten(f1_scores))\n",
    "    \n",
    "    # precision-recall curve only for one document \n",
    "    example_category = 0\n",
    "    example_article = 1\n",
    "    precision_c, recall_c = get_precision_recall_curve_data(R[example_category][example_article], S[example_category][example_article])\n",
    "    plot_precision_recall_curve(precision_c, recall_c)\n",
    "    mAP = statistics.mean(precision_c)\n",
    "    print(f\"Mean average precision: {mAP}\")\n",
    "    metrics = {'mean_precision': mean_precision, 'mean_recall': mean_recall, 'mean_f1_scores': mean_f1_scores}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing/reference/categorized_summary_sentence_indices.json\", \"r\") as f:\n",
    "    categorized_summary_sentence_indices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing/extracts/tf_idf/categorized_scores.json\", \"r\") as f: \n",
    "    tf_idf_extract = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testing/extracts/bm25/categorized_scores.json\", \"r\") as f: \n",
    "    bm25_extract = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(categorized_summary_sentence_indices, tf_idf_extract, category_names=category_names, model_name=\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(categorized_summary_sentence_indices, bm25_extract, category_names=category_names, model_name=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset for BERT comparison\\\n",
    "takes too long otherwise\\ \n",
    "\n",
    "TODO: \n",
    "* evaluation + first question -> Sebastian   \n",
    "* question 2, 3, 4 -> Francisco\n",
    "* question 5 and 6 -> Tuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "summarization(article_id, p=5, l=100, o=\"rel\", I_or_D=I) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RRF(article_id, index, collection, mu=5, p=5):\n",
    "    \n",
    "    #get all scores for BERT\n",
    "    scores_bert = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=collection, model='BERT', bert_model=bert_model, bert_tokenizer=bert_tokenizer)\n",
    "    ranks_bert = {key: rank for rank, key in enumerate(scores_bert, 1)}\n",
    "    #get all scores of BM25\n",
    "    scores_bm25 = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=index, model='BM25')\n",
    "    ranks_bm25 = {key: rank for rank, key in enumerate(scores_bm25, 1)} \n",
    "    \n",
    "    #get RRF\n",
    "    scores_rrf = defaultdict(int)\n",
    "    for sent_number in ranks_bert.keys():\n",
    "        scores_rrf[sent_number] = 1/(mu + ranks_bert[sent_number]) + 1/(mu + ranks_bm25[sent_number])\n",
    "        \n",
    "    return sort_by_value(scores_rrf, p, reverse=True)\n",
    "\n",
    "article_id = map_path_to_articleID(document_path)\n",
    "print(\"ORIGINAL DOCUMENT\")\n",
    "print(articles[article_id])\n",
    "all_scores = get_RRF (article_id, I, articles, mu=5, p=5)\n",
    "all_scores = dict(sorted(all_scores.items()))\n",
    "print(\"SUMMARY\")\n",
    "sentences = nltk.sent_tokenize(articles[article_id])\n",
    "for sent_id, rank in all_scores.items(): \n",
    "    print(f\"{rank:.2f}: {sentences[sent_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMR \n",
    "Maximal Marginal Relevance (MRR) is a method that can be used to increase sentence diversity. \n",
    "\n",
    "The proposed method iteratively selects the most informative sentence according to a given IR model, adding it to the set of sentences in the summary and removing it from the document. Next sentence selection is based on the MMR score that simultaneously attempts to select sentences that are relevant and dissimilar to the sentences selected so far (non-redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "summarization(10, 0, 0, 'rel', I, model='MMR-TFIDF', λ=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "summarization(10, 0, 0, 'rel', I, model='MMR-TFIDF', λ=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Classification Metrics Over P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def get_conf_matrix_stats(article_file_paths, index, collection, summaries):\n",
    "    conf_stats = []\n",
    "    \n",
    "    for category_id, category in enumerate(article_file_paths): \n",
    "        for path in category: \n",
    "            article_id = map_path_to_articleID(path)\n",
    "            article = nltk.sent_tokenize(articles[article_id])\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            #Calculating scores for all sentences using summarization function\n",
    "            scores_bm25 = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=index, model='BM25')\n",
    "            scores_bm25= sort_by_value(scores_bm25, 1000, reverse=True)\n",
    "            scores_tfidf = summarization(d=article_id, p=0, l=0, o=\"rel\", I_or_D=index, model='TF-IDF')\n",
    "            scores_tfidf = sort_by_value(scores_tfidf, 1000, reverse=True)\n",
    "\n",
    "            #Now we are iterating over different p to see how our summarization results under different p\n",
    "            for p in range(16):        \n",
    "                if p==0:\n",
    "                    continue\n",
    "\n",
    "                #initializing tp, fp, tn, fn for the two models    \n",
    "                tp_tfidf, fp_tfidf, tn_tfidf, fn_tfidf = 0, 0, 0, 0\n",
    "                tp_bm25, fp_bm25, tn_bm25, fn_bm25 = 0, 0, 0, 0\n",
    "\n",
    "                #iterating over all sentences in our articles to classify them as tp,fp,tn,fn\n",
    "                for key in range(len(article)):\n",
    "\n",
    "                    #TP, FP, TN, FN for BM-25  \n",
    "                    if (key in dict(itertools.islice(scores_bm25.items(), p)).keys()) & (article[key] in summaries[article_id]):\n",
    "                        tp_bm25 +=1     \n",
    "                    elif (key not in dict(itertools.islice(scores_bm25.items(), p)).keys()) & (article[key] in summaries[article_id]):\n",
    "                        fn_bm25 +=1\n",
    "                    elif (key in dict(itertools.islice(scores_bm25.items(), p)).keys()) & (article[key] not in summaries[article_id]):\n",
    "                        fp_bm25 +=1\n",
    "                    else:\n",
    "                        tn_bm25 +=1\n",
    "\n",
    "                    #TP, FP, TN, FN for TF-IDF\n",
    "                    if (key in dict(itertools.islice(scores_tfidf.items(), p)).keys()) & (article[key] in summaries[article_id]):\n",
    "                        tp_tfidf +=1     \n",
    "                    elif (key not in dict(itertools.islice(scores_tfidf.items(), p)).keys()) & (article[key] in summaries[article_id]):\n",
    "                        fn_tfidf +=1\n",
    "                    elif (key in dict(itertools.islice(scores_tfidf.items(), p)).keys()) & (article[key] not in summaries[article_id]):\n",
    "                        fp_tfidf +=1\n",
    "                    else:\n",
    "                        tn_tfidf +=1 \n",
    "                \n",
    "                \n",
    "                acc_bm25 = (tp_bm25 + tn_bm25)/ (tp_bm25 + fn_bm25 + tn_bm25 + fp_bm25)\n",
    "                precision_bm25 = tp_bm25/ (tp_bm25 + fp_bm25)\n",
    "                recall_bm25 = tp_bm25/ (tp_bm25 + fn_bm25)\n",
    "                f1_bm25 = 0 if precision_bm25+recall_bm25==0 else 2 * (precision_bm25*recall_bm25) / (precision_bm25+recall_bm25)\n",
    "                specificity_bm25 = tn_bm25/ (tn_bm25 + fp_bm25)\n",
    "                \n",
    "                acc_tfidf = (tp_tfidf + tn_tfidf)/ (tp_tfidf + fn_tfidf + tn_tfidf + fp_tfidf)\n",
    "                precision_tfidf = tp_tfidf/ (tp_tfidf + fp_tfidf)\n",
    "                recall_tfidf = tp_tfidf/ (tp_tfidf + fn_tfidf)\n",
    "                f1_tfidf = 0 if precision_tfidf+recall_tfidf==0 else 2 * (precision_tfidf*recall_tfidf) / (precision_tfidf+recall_tfidf)\n",
    "                specificity_tfidf = tn_tfidf/ (tn_tfidf + fp_tfidf)\n",
    "                \n",
    "                #append stats\n",
    "                conf_stats.append({'article': article_id , 'p': p,\n",
    "                                   'tp_bm25':tp_bm25, 'fn_bm25':fn_bm25, 'fp_bm25':fp_bm25, 'tn_bm25':tn_bm25,\n",
    "                                   'acc_bm25':acc_bm25, 'precision_bm25':precision_bm25, 'recall_bm25':recall_bm25,\n",
    "                                   'f1_bm25':f1_bm25, 'specificity_bm25':specificity_bm25,\n",
    "                                   'tp_tfidf':tp_tfidf, 'fn_tfidf':fn_tfidf, 'fp_tfidf':fp_tfidf, 'tn_tfidf':tn_tfidf,\n",
    "                                   'acc_tfidf':acc_tfidf, 'precision_tfidf':precision_tfidf, 'recall_tfidf':recall_tfidf,\n",
    "                                   'f1_tfidf':f1_tfidf, 'specificity_tfidf':specificity_tfidf})\n",
    "                #print(\"article:\", article_id, \"for\", p, \"-length is done!\")\n",
    "    #print(\"time spent is:\", time.time() - start_time)           \n",
    "    return conf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conf_stats_over_p = get_conf_matrix_stats(article_file_paths, I, articles, summaries)\n",
    "df_art_p = pd.DataFrame.from_dict(conf_stats_over_p, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_art_p['acc_bm25'] = (df_art_p['tp_bm25'] + df_art_p['tn_bm25'])/ (df_art_p['tp_bm25'] + df_art_p['fn_bm25'] + df_art_p['tn_bm25'] + df_art_p['fp_bm25'])\n",
    "df_art_p['precision_bm25'] = df_art_p['tp_bm25']/ (df_art_p['tp_bm25'] + df_art_p['fp_bm25'])\n",
    "df_art_p['recall_bm25'] = df_art_p['tp_bm25']/ (df_art_p['tp_bm25'] + df_art_p['fn_bm25'])\n",
    "df_art_p['f1_bm25'] = df_art_p.apply(lambda x: 0 if (x['precision_bm25'] + x['recall_bm25'])==0 else 2 * (x['precision_bm25']*x['recall_bm25']) / (x['precision_bm25']+x['recall_bm25']),  axis=1)\n",
    "df_art_p['specificity_bm25'] = df_art_p['tn_bm25']/ (df_art_p['tn_bm25'] + df_art_p['fp_bm25'])\n",
    "\n",
    "df_art_p['acc_tfidf'] = (df_art_p['tp_tfidf'] + df_art_p['tn_tfidf'])/ (df_art_p['tp_tfidf'] + df_art_p['fn_tfidf'] + df_art_p['tn_tfidf'] + df_art_p['fp_tfidf'])\n",
    "df_art_p['precision_tfidf'] = df_art_p['tp_tfidf']/ (df_art_p['tp_tfidf'] + df_art_p['fp_tfidf'])\n",
    "df_art_p['recall_tfidf'] = df_art_p['tp_tfidf']/ (df_art_p['tp_tfidf'] + df_art_p['fn_tfidf'])\n",
    "df_art_p['f1_tfidf'] = df_art_p.apply(lambda x: 0 if (x['precision_tfidf'] + x['recall_tfidf'])==0 else 2 * (x['precision_tfidf']*x['recall_tfidf']) / (x['precision_tfidf']+x['recall_tfidf']),  axis=1)\n",
    "df_art_p['specificity_tfidf'] = df_art_p['tn_tfidf']/ (df_art_p['tn_tfidf'] + df_art_p['fp_tfidf'])\n",
    "\n",
    "df_art_p.plot(x='p', y=['precision_bm25', 'recall_bm25', 'f1_bm25'], title='BM-25 Performance Over The Collection',  grid=True, xticks=df_art_p['p'])\n",
    "df_art_p.plot(x='p', y=['precision_tfidf', 'recall_tfidf', 'f1_tfidf'], title='TF-IDF Performance Over The Collection',  grid=True, xticks=df_art_p['p'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "article_id = 0\n",
    "print(articles[article_id])\n",
    "keyword_extraction(article_id, 10, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_sentences(documents: list) -> list: \n",
    "    number_of_sentences = list()\n",
    "    for document in documents: \n",
    "        sents = sent_tokenize(document) \n",
    "        number_of_sentences.append(len(sents))\n",
    "    return number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(documents: list) -> Union[float, float, float]: \n",
    "    number_of_sentences = get_number_of_sentences(documents)\n",
    "    mean = statistics.mean(number_of_sentences)\n",
    "    median = statistics.median(number_of_sentences)\n",
    "    std = statistics.stdev(number_of_sentences)\n",
    "    return mean, std, median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(documents: list):\n",
    "    number_of_sentences = get_number_of_sentences(documents)\n",
    "    mean, std, median = get_statistics(documents)\n",
    "    plt.hist(number_of_sentences, bins=80)\n",
    "    plt.title(f\"mean: {mean:.2f}, std: {std:.2f}, median: {median:.2f}\")\n",
    "    plt.xlabel(\"Number of sentences\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "\n",
    "# average number of sentences \n",
    "mean, std, median = get_statistics(articles)\n",
    "plot_distribution(articles)\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")\n",
    "print(f\"Median: {median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mean, std, median = get_statistics(summaries)\n",
    "#plot_distribution(summaries)\n",
    "#print(f\"Mean: {mean}\")\n",
    "##print(f\"Standard deviation: {std}\")\n",
    "#print(f\"Median: {median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = [len(l) for l in summary_sentence_indices]\n",
    "mean = statistics.mean(number_of_sentences)\n",
    "median = statistics.median(number_of_sentences)\n",
    "std = statistics.stdev(number_of_sentences)\n",
    "plt.hist(number_of_sentences, bins=80)\n",
    "plt.title(f\"mean: {mean:.2f}, std: {std:.2f}, median: {median:.2f}\")\n",
    "plt.xlabel(\"Number of sentences\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
